{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Neural_Networks_from_Scratch_using_NumPy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viVefdvcN69F",
        "colab_type": "text"
      },
      "source": [
        "#**IMPORT REQUIRED PACKAGES**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtUemp3HN35X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLiHm83vOV8R",
        "colab_type": "text"
      },
      "source": [
        "#**DEEP NEURAL NETWORK CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZRGDIXZQC35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep_Neural_Network :\n",
        "\n",
        "  def create(self,input_size,output_size,hidden_dims,output_type,initializer='random',\n",
        "             seed=None,activation='relu',leaky_relu_slope=0.1):\n",
        "    \"\"\"\n",
        "    The method to define the architecture of Deep Neural Network and initialize weights.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    input_size(int)       :   No. of neurons in input layer.\n",
        "    \n",
        "    output_size(int)      :   No. of classes in classification task (2 in case of binary classification,\n",
        "                              modify the dataset accordingly !)\n",
        "                              (or) No. of Target variables in case of regression task.\n",
        "\n",
        "    hidden_dims(int list) :   No. of neurons in hidden layers.\n",
        "\n",
        "    output_type(string)   :   Type of task :\n",
        "                              'classification'  :  Classification (discrete target).\n",
        "                              'regression'      :  Regression (continuous target).\n",
        "\n",
        "    initializer(string)   :   Weight initializer :\n",
        "                              'random'  : Random initialization.\n",
        "                              'xavier'  : Xavier initialization (preferred for tanh activation).\n",
        "                              'he'      : He initialization (preferred for ReLU activation).\n",
        "\n",
        "    seed(int)             :   NumPy seed for random initialization.\n",
        "\n",
        "    activation(string)    :   Activation function for hidden layers. One of the following :\n",
        "                              'linear'  : Linear activation.\n",
        "                              'sigmoid' : Sigmoid activation.\n",
        "                              'tanh'    : Hyberbolic tangent activation.\n",
        "                              'relu'    : Rectified Linear Unit activation.\n",
        "                              'lrelu'   : Leaky Rectified Linear Unit activation.\n",
        "\n",
        "                              Activation function at output layer would be SoftMax for classification\n",
        "                              and Linear for regression.\n",
        "\n",
        "    leaky_relu_slope(int) :   Slope for Leaky ReLU activation.   \n",
        "\n",
        "    \"\"\"\n",
        "    self.layer_dims=[input_size]+hidden_dims+[output_size]\n",
        "    self.W = {}\n",
        "    self.b = {}\n",
        "    self.activation = activation\n",
        "    self.leaky_relu_slope = leaky_relu_slope\n",
        "    self.initializer = initializer\n",
        "    self.output_type = output_type\n",
        "\n",
        "    self.L = len(self.layer_dims)-1\n",
        "\n",
        "    if seed != None:\n",
        "      np.random.seed(seed)\n",
        "\n",
        "    for i in range(self.L):\n",
        "      self.W[i+1] = np.random.randn(self.layer_dims[i+1],self.layer_dims[i])\n",
        "      self.b[i+1] = np.zeros((self.layer_dims[i+1],1))\n",
        "\n",
        "    if self.initializer == 'xavier':\n",
        "      for i in range(self.L):\n",
        "        self.W[i+1] = self.W[i+1]*np.sqrt(1/(self.layer_dims[i]))\n",
        "\n",
        "    elif self.initializer == 'he':\n",
        "      for i in range(self.L):\n",
        "        self.W[i+1] = self.W[i+1]*np.sqrt(2/(self.layer_dims[i]))\n",
        "\n",
        "\n",
        "\n",
        "  def save_weights(self):\n",
        "    \"\"\"\n",
        "    The method to save model weights.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    params(tuple)   : Tuple containing Model weights and biases in form of dictionaries.\n",
        "\n",
        "    \"\"\"\n",
        "    return (self.W,self.b)\n",
        "\n",
        "  \n",
        "\n",
        "  def load_weights(self,params):\n",
        "    \"\"\"\n",
        "    The method to load model weights.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    params(tuple)   : Tuple containing Model weights and biases in form of dictionaries.\n",
        "\n",
        "    \"\"\"\n",
        "    self.W = params[0]\n",
        "    self.b = params[1]\n",
        "\n",
        "\n",
        "\n",
        "  ### ACTIVATION FUNCTIONS AND THEIR GRADIENTS ###\n",
        "\n",
        "  def linear(self,X):\n",
        "    return X\n",
        "\n",
        "  def linear_grad(self,X):\n",
        "    return np.ones(shape=X.shape) \n",
        "\n",
        "  def sigmoid(self,X):\n",
        "    return 1/(1+np.exp(-X))\n",
        "\n",
        "  def sigmoid_grad(self,X):\n",
        "    return self.sigmoid(X)*(1-self.sigmoid(X))\n",
        "\n",
        "  def tanh(self,X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "  def tanh_grad(self,X):\n",
        "    return 1-((self.tanh(X))**2)\n",
        "\n",
        "  def relu(self,X):\n",
        "    return np.maximum(0,X)\n",
        "\n",
        "  def relu_grad(self,X):\n",
        "    return 1.0*(X>0)\n",
        "\n",
        "  def lrelu(self,X):\n",
        "    return np.where(X > 0, X, X * self.leaky_relu_slope)\n",
        "\n",
        "  def lrelu_grad(self,X):\n",
        "    return np.where(X > 0, 1, self.leaky_relu_slope)   \n",
        "\n",
        "  def softmax(self,X):\n",
        "    exps = np.exp(X-np.max(X))\n",
        "    return exps/np.sum(exps,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "  def to_one_hot(self,X):\n",
        "    \"\"\"\n",
        "    The method to convert SoftMax probabilities to label in one hot form.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    X(NumPy 2D array of shape (output_size,m))  : Predicted SoftMax probabilities for batch of size m.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    X_one_hot(NumPy 2D array of shape (output_size,m)) : Predicted labels in one hot form for batch of \n",
        "                                                         size m.\n",
        "\n",
        "    \"\"\"\n",
        "    a = np.argmax(X,axis=0)\n",
        "    b = np.zeros((X.shape[0],a.size))\n",
        "    b[a,np.arange(a.size)] = 1\n",
        "    return b\n",
        "\n",
        "\n",
        "\n",
        "  def accuracy(self,Y_pred,Y_true):\n",
        "    \"\"\"\n",
        "    The method to calculate classification accuracy.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    Y_pred(NumPy 2D array of shape (output_size,m))  : Predicted labels for batch of size m.\n",
        "    \n",
        "    Y_true(NumPy 2D array of shape (output_size,m))  : Actual labels for batch of size m.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    accuracy(float)   : Accuracy in range [0,1].\n",
        "\n",
        "    \"\"\"\n",
        "    a = np.argmax(Y_pred,axis=0)\n",
        "    b = np.argmax(Y_true,axis=0)\n",
        "\n",
        "    correct = np.sum((a==b)*1)\n",
        "    total = a.size\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "\n",
        "\n",
        "  def forward_propagation(self,X,dropout=False):\n",
        "    \"\"\"\n",
        "    The method to forward propagate input data through the network, and calculate activations of \n",
        "    each layer.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    X(NumPy 2D array of shape (input_size,m))  : Input data for batch of size m.\n",
        "\n",
        "    dropout(boolean)                           : Perform dropout or not.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    activations(dictionary)   : Activations of all layers.\n",
        "\n",
        "    \"\"\"\n",
        "    self.Z = {}\n",
        "    self.A = {}\n",
        "\n",
        "    self.A[0] = X\n",
        "\n",
        "    for i in range (len(self.layer_dims)-2):\n",
        "      self.Z[i+1] = np.matmul(self.W[i+1],self.A[i])+self.b[i+1]\n",
        "      _ = \"self.A[i+1] = self.\"+self.activation+\"(self.Z[i+1])\"\n",
        "      exec(_)\n",
        "\n",
        "      if dropout == True:\n",
        "        self.A[i+1] *= ((np.random.rand(self.A[i+1].shape[0],self.A[i+1].shape[1])<self.keep_probs[i])*1)\n",
        "        self.A[i+1].reshape(1,-1)\n",
        "        self.A[i+1] /= self.keep_probs[i]\n",
        "\n",
        "    self.Z[self.L] = np.matmul(self.W[self.L],self.A[len(self.layer_dims)-2])+self.b[self.L]\n",
        "\n",
        "    if self.output_type == 'classification':\n",
        "      self.A[self.L] = self.softmax(self.Z[self.L])\n",
        "\n",
        "    elif self.output_type == 'regression':\n",
        "      self.A[self.L] = self.Z[self.L] \n",
        "\n",
        "    return self.A\n",
        "\n",
        "\n",
        "\n",
        "  def compute_cost(self,Y_pred,Y_true):\n",
        "    \"\"\"\n",
        "    The method to compute cost for the current forward propagated batch.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    Y_pred(NumPy 2D array of shape (output_size,m))  : Predicted outputs for current forward propagated \n",
        "                                                       batch of size m.\n",
        "    \n",
        "    Y_true(NumPy 2D array of shape (output_size,m))  : Ground truths for current forward propagated batch\n",
        "                                                       of size m.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    cost(float)   : Cost for current forward propagated batch.\n",
        "\n",
        "    \"\"\"\n",
        "    if self.output_type=='classification':\n",
        "      cost = (1/Y_true.shape[1])*np.sum(-1*Y_true*np.log(Y_pred))\n",
        "\n",
        "    else:\n",
        "      cost = (1/(2*Y_true.shape[1]))*np.sum((Y_pred-Y_true)**2)\n",
        "\n",
        "    sum=0\n",
        "\n",
        "    if self.regularizer == 'l2':\n",
        "      for i in range(self.L):\n",
        "        sum += np.sum(self.W[i+1]**2)\n",
        "      cost += ((1/(2*Y_true.shape[1]))*sum)      \n",
        "\n",
        "    elif self.regularizer == 'l1':\n",
        "      for i in range(self.L):\n",
        "        sum += np.sum(np.abs(self.W[i+1]))\n",
        "      cost += ((1/Y_true.shape[1])*sum)   \n",
        "    \n",
        "    return cost\n",
        "\n",
        "\n",
        "\n",
        "  def backward_propagation(self,Y):\n",
        "    \"\"\"\n",
        "    The method to compute gradient of cost with respect to weights and biases of each layer.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    Y(NumPy 2D array of shape (output_size,m))  : Ground truths for current forward propagated batch \n",
        "                                                  of size m.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    gradients(tuple)   : Tuple containing gradients of cost with respect to weights and biases of each \n",
        "                         layer in form of dictionaries.\n",
        "\n",
        "    \"\"\"\n",
        "    self.dZ = {}\n",
        "    self.dA = {}\n",
        "    self.dW = {}\n",
        "    self.db = {}\n",
        "\n",
        "    self.dZ[self.L] = self.A[self.L]-Y\n",
        "\n",
        "    for i in range(self.L,0,-1):\n",
        "\n",
        "      self.dW[i] = (1/self.dZ[i].shape[1])*np.matmul(self.dZ[i],self.A[i-1].T)\n",
        "\n",
        "      if self.regularizer == 'l2':\n",
        "        self.dW[i] += self.regularizer_lambda*self.W[i]\n",
        "      elif self.regularizer == 'l1':\n",
        "        self.dW[i] += self.regularizer_lambda*np.where(self.W[i]>0,1,-1) \n",
        "        \n",
        "      self.db[i] = (1/self.dZ[i].shape[1])*np.sum(self.dZ[i],axis=1,keepdims=True)\n",
        "      _ = \"self.dZ[i-1] = np.matmul(self.W[i].T,self.dZ[i])*self.\"+self.activation+\"_grad(self.A[i-1])\"\n",
        "      exec(_) \n",
        "      \n",
        "    return (self.dW,self.db)\n",
        "  \n",
        "\n",
        "\n",
        "  def train(self,X_train,Y_train,X_val,Y_val,optimizer='vanilla',regularizer=None,regularizer_lambda=0.02,\n",
        "            keep_probs=[],mini_batch_size=32,epochs=100,learning_rate=0.01,beta=0.9,beta1=0.9,beta2=0.99,\n",
        "            print_loss_freq=100,plot_loss=True):\n",
        "    \"\"\"\n",
        "    The method to train the weights and biases of each layer for the provided training data with \n",
        "    ground truths.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    X_train(NumPy 2D array of shape(input_size,m))   :  Input data(for batch of size m) for training.\n",
        "\n",
        "    Y_train(NumPy 2D array of shape(output_size,m))  :  Ground truths(for batch of size m) for training.\n",
        "\n",
        "    X_val(NumPy 2D array of shape(input_size,m))     :  Input data(for batch of size m) for validation.\n",
        "\n",
        "    Y_val(NumPy 2D array of shape(output_size,m))    :  Ground truths(for batch of size m) for validation.\n",
        "\n",
        "    optimizer(string)             :   Optimizer for training process, one of the following :\n",
        "                                      'vanilla'     : Original gradient decsent.\n",
        "                                      'momentum'    : Gradient descent with momentum.\n",
        "                                      'rmsprop'     : Root mean square propagation.\n",
        "                                      'adam'        : Adaptive moments estimation.\n",
        "    \n",
        "    regularizer(string)           :   Regularizer for weights of network, one of the following :\n",
        "                                      'l1'      : L1 regularization.\n",
        "                                      'l2'      : L2 regularization.\n",
        "                                      'dropout' : Dropout regularization.\n",
        "                                      None      : No regularizer.\n",
        "\n",
        "    regularizer_lambda(float)     :   Regularization parameter lambda for L1 or L2 regularization.\n",
        "\n",
        "    keep_probs(float[0,1] list)   :   Keeping probabilities for hidden layers in Dropout regularization.\n",
        "\n",
        "    mini_batch_size(int)          :   Mini Batch size (1 for Stochastic gradient descent).\n",
        "\n",
        "    epochs(int)                   :   No. of iterations over training set.\n",
        "\n",
        "    learning_rate(float)          :   Learning rate aplha.\n",
        "\n",
        "    beta(float)                   :   Optimizer parameter beta for 'momentum' and 'rmsprop' optimizers.\n",
        "\n",
        "    beta1(float)                  :   Optimizer parameter beta2 for 'adam' optimizer.\n",
        "\n",
        "    beta2(float)                  :   Optimizer parameter beta2 for 'adam' optimizer.\n",
        "\n",
        "    print_loss_freq(int)          :   Frequency of printing metrics.\n",
        "\n",
        "    plot_loss(boolean)            :   Plot learning curves or not.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    Metrics_history(tuple)        :   History of metrics in form of lists\n",
        "\n",
        "    \"\"\"\n",
        "    self.regularizer_lambda = regularizer_lambda\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "    if keep_probs != []:\n",
        "      self.keep_probs = keep_probs\n",
        "    else:\n",
        "      self.keep_probs = [1]*(len(self.layer_dims)-2)\n",
        "\n",
        "    self.print_loss_freq = print_loss_freq  \n",
        "\n",
        "    self.Mw = {}\n",
        "    self.Mb = {}\n",
        "    self.Vw = {}\n",
        "    self.Vb = {}\n",
        "\n",
        "    for i in range(self.L):\n",
        "      self.Mw[i+1] = np.zeros(shape=self.W[i+1].shape)\n",
        "      self.Mb[i+1] = np.zeros(shape=self.b[i+1].shape)\n",
        "      self.Vw[i+1] = np.zeros(shape=self.W[i+1].shape)\n",
        "      self.Vb[i+1] = np.zeros(shape=self.b[i+1].shape)\n",
        "\n",
        "    train_cost = []\n",
        "    val_cost = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    m = X_train.shape[1]\n",
        "\n",
        "    drop = False\n",
        "    if(self.regularizer == 'dropout'):\n",
        "      drop = True\n",
        "\n",
        "    t = 1\n",
        "    \n",
        "    for e in range(epochs):\n",
        "\n",
        "      mask = np.random.permutation(m)\n",
        "\n",
        "      X_train = X_train[:,mask]\n",
        "      Y_train = Y_train[:,mask]\n",
        "\n",
        "      if optimizer == 'vanilla':\n",
        "\n",
        "        for i in range(0,m,mini_batch_size):\n",
        "\n",
        "          _ = self.forward_propagation(X_train[:,i:(i+mini_batch_size)],drop)\n",
        "          _ = self.backward_propagation(Y_train[:,i:(i+mini_batch_size)])\n",
        "          \n",
        "          for i in range(self.L):\n",
        "            self.W[i+1] -= learning_rate*self.dW[i+1]\n",
        "            self.b[i+1] -= learning_rate*self.db[i+1]\n",
        "\n",
        "      elif optimizer == 'momentum':\n",
        "\n",
        "        for i in range(0,m,mini_batch_size):\n",
        "\n",
        "          _ = self.forward_propagation(X_train[:,i:(i+mini_batch_size)],drop)\n",
        "          _ = self.backward_propagation(Y_train[:,i:(i+mini_batch_size)])\n",
        "\n",
        "          for i in range(self.L):\n",
        "            self.Mw[i+1] = (beta*self.Mw[i+1])+(1-beta)*self.dW[i+1]\n",
        "            self.Mb[i+1] = (beta*self.Mb[i+1])+(1-beta)*self.db[i+1]\n",
        "          \n",
        "          \n",
        "          for i in range(self.L):\n",
        "            self.W[i+1] -= learning_rate*self.Mw[i+1]\n",
        "            self.b[i+1] -= learning_rate*self.Mb[i+1]\n",
        "\n",
        "      elif optimizer == 'rmsprop':\n",
        "\n",
        "        for i in range(0,m,mini_batch_size):\n",
        "\n",
        "          _ = self.forward_propagation(X_train[:,i:(i+mini_batch_size)],drop)\n",
        "          _ = self.backward_propagation(Y_train[:,i:(i+mini_batch_size)])\n",
        "\n",
        "          for i in range(self.L):\n",
        "            self.Vw[i+1] = (beta*self.Vw[i+1])+(1-beta)*(self.dW[i+1]**2)\n",
        "            self.Vb[i+1] = (beta*self.Vb[i+1])+(1-beta)*(self.db[i+1]**2)\n",
        "          \n",
        "          for i in range(self.L):\n",
        "            \n",
        "            self.W[i+1] -= learning_rate*(self.dW[i+1]/(np.sqrt(self.Vw[i+1])+10e-8))\n",
        "            self.b[i+1] -= learning_rate*(self.db[i+1]/(np.sqrt(self.Vb[i+1])+10e-8))\n",
        "\n",
        "      elif optimizer == 'adam':\n",
        "        \n",
        "        for i in range(0,m,mini_batch_size):\n",
        "\n",
        "          _ = self.forward_propagation(X_train[:,i:(i+mini_batch_size)],drop)\n",
        "          _ = self.backward_propagation(Y_train[:,i:(i+mini_batch_size)])\n",
        "\n",
        "          for i in range(self.L):\n",
        "\n",
        "            self.Mw[i+1] = (beta1*self.Mw[i+1])+(1-beta1)*self.dW[i+1]\n",
        "            #self.Mw[i+1] /= (1-np.power(beta1,t))\n",
        "            self.Mb[i+1] = (beta1*self.Mb[i+1])+(1-beta1)*self.db[i+1]\n",
        "            #self.Mb[i+1] /= (1-np.power(beta1,t))\n",
        "\n",
        "          for i in range(self.L):\n",
        "            self.Vw[i+1] = (beta2*self.Vw[i+1])+(1-beta2)*(self.dW[i+1]**2)\n",
        "            #self.Vw[i+1] /= (1-np.power(beta2,t))\n",
        "            self.Vb[i+1] = (beta2*self.Vb[i+1])+(1-beta2)*(self.db[i+1]**2)\n",
        "            #self.Vb[i+1] /= (1-np.power(beta2,t))\n",
        "\n",
        "          t += 1  \n",
        "\n",
        "          for i in range(self.L):\n",
        "            self.W[i+1] -= learning_rate*(self.Mw[i+1]/(np.sqrt(self.Vw[i+1])+10e-8))\n",
        "            self.b[i+1] -= learning_rate*(self.Mb[i+1]/(np.sqrt(self.Vb[i+1])+10e-8))\n",
        "\n",
        "      Y_pred_train = self.forward_propagation(X_train)[self.L]\n",
        "      Y_pred_val = self.forward_propagation(X_val)[self.L]\n",
        "                                   \n",
        "      train_cost.append(self.compute_cost(Y_pred_train,Y_train))\n",
        "      val_cost.append(self.compute_cost(Y_pred_val,Y_val))\n",
        "\n",
        "      train_acc.append(self.accuracy(self.to_one_hot(Y_pred_train),Y_train))\n",
        "      val_acc.append(self.accuracy(self.to_one_hot(Y_pred_val),Y_val))\n",
        "\n",
        "      if (e+1)%self.print_loss_freq==0:\n",
        "        if self.output_type == 'classification':\n",
        "          print(\"After \"+str(e+1)+\" epochs :    Training Loss = \"+str(train_cost[e]) +\n",
        "                \"    Validation Loss = \"+str(val_cost[e]) + \"\\n\\n\\t\\t \" +\n",
        "                \"    Training Accuracy = \"+str(train_acc[e]) +\n",
        "                \"    Validation Accuracy = \"+str(val_acc[e])+'\\n')\n",
        "        else:\n",
        "          print(\"After \"+str(e+1)+\" epochs :    Training Loss = \"+str(train_cost[e]) + \n",
        "                \"    Validation Loss = \"+str(val_cost[e])+'\\n')  \n",
        "\n",
        "    if plot_loss == True:\n",
        "\n",
        "      r = list(range(1,epochs+1))\n",
        "      plt.plot(r,train_cost,'r',label=\"Training Loss\")\n",
        "      plt.plot(r,val_cost,'b',label=\"Validation Loss\")\n",
        "      plt.xlabel('Epochs')\n",
        "      if self.output_type == 'regression':\n",
        "        plt.ylabel('L2 Loss')\n",
        "      else:\n",
        "        plt.ylabel('Categorical Cross Entropy Loss') \n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "      print(\"\\nTraining Loss : \",train_cost[-1])\n",
        "      print(\"\\nValidation Loss : \",val_cost[-1]) \n",
        "\n",
        "      if self.output_type == 'classification':\n",
        "        print(\"\\nTraining Accuracy : \",train_acc[-1])\n",
        "        print(\"\\nValidation Accuracy : \",val_acc[-1]) \n",
        "\n",
        "    return (train_cost,val_cost,train_acc,val_acc)       \n",
        "\n",
        "\n",
        "\n",
        "  def predict(self,X):\n",
        "    \"\"\"\n",
        "    The method to predict outputs for given unknown input data.\n",
        "\n",
        "    Parameters :\n",
        "\n",
        "    X(NumPy 2D array of shape (input_size,m))  : Input data for batch of size m.\n",
        "\n",
        "    Returns :\n",
        "\n",
        "    Y_pred(NumPy 2D array of shape (output_size_size,m))  : Predicted output for batch of size m.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    A = self.forward_propagation(X)\n",
        "\n",
        "    if self.output_type == 'regression':\n",
        "      return A[self.L]\n",
        "    else:\n",
        "      return self.to_one_hot(A[self.L])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRcJBBFr8ux2",
        "colab_type": "text"
      },
      "source": [
        "#**CLASSIFICATION on MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnl_y2m3VFFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wS_Thst1YSi",
        "colab_type": "code",
        "outputId": "4f0166b4-91fc-4770-c7f6-439422c5b39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load MNIST dataset \n",
        "\n",
        "(X, Y), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkmqvmQw-jPM",
        "colab_type": "code",
        "outputId": "0b58826c-bad2-4aca-cd15-3c676f7d537e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Visualize the dataset\n",
        "\n",
        "index = 300\n",
        "\n",
        "plt.imshow(X[index],cmap='gray')\n",
        "plt.show\n",
        "\n",
        "print('Label = '+str(Y[index]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label = 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql27Xvz19LAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize, reshape and split into train, test and validation datasets.\n",
        "\n",
        "X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.01)\n",
        "Y_train = to_categorical(Y_train).T\n",
        "X_train = X_train.T.reshape(784,-1)/255\n",
        "Y_val = to_categorical(Y_val).T\n",
        "X_val = X_val.T.reshape(784,-1)/255\n",
        "Y_test = to_categorical(Y_test).T\n",
        "X_test = X_test.T.reshape(784,-1)/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUFwHTvMmFVm",
        "colab_type": "code",
        "outputId": "31ca8f20-3495-4d3a-a152-de99f94fcc0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "source": [
        "# Define and train the Deep Neural Network.\n",
        "\n",
        "model = Deep_Neural_Network()\n",
        "model.create(784,10,[800],output_type='classification',activation='lrelu',initializer='he',\n",
        "             leaky_relu_slope=0.1)\n",
        "\n",
        "costs = model.train(X_train,Y_train,X_val,Y_val,optimizer='adam',regularizer='dropout',keep_probs=[0.75],\n",
        "                    mini_batch_size=128,epochs=20,print_loss_freq=5,learning_rate=0.0002)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 5 epochs :    Training Loss = 0.09741785817154956    Validation Loss = 0.11765183547030508\n",
            "\n",
            "\t\t     Training Accuracy = 0.9734680134680135    Validation Accuracy = 0.97\n",
            "\n",
            "After 10 epochs :    Training Loss = 0.04905321969145776    Validation Loss = 0.07825224710500901\n",
            "\n",
            "\t\t     Training Accuracy = 0.9867508417508417    Validation Accuracy = 0.975\n",
            "\n",
            "After 15 epochs :    Training Loss = 0.028190736743280063    Validation Loss = 0.06407545699787537\n",
            "\n",
            "\t\t     Training Accuracy = 0.9934006734006734    Validation Accuracy = 0.985\n",
            "\n",
            "After 20 epochs :    Training Loss = 0.01690642379925382    Validation Loss = 0.0629027117189508\n",
            "\n",
            "\t\t     Training Accuracy = 0.996986531986532    Validation Accuracy = 0.9883333333333333\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3iUZfbw8e8JSQgkoWOhBgSF0EME\nRYoIIokC4rICa0EsqK/dlZV1FbHtquvay6q/1bWCiouAUkRAxUqTIk06UkRA6TXhvH/cT0hhkjwk\nU1LO57qeKzNPm5NhmJO7i6pijDHG5BUV6QCMMcaUTJYgjDHGBGQJwhhjTECWIIwxxgRkCcIYY0xA\n0ZEOIFhq1aqlSUlJkQ7DGGNKlXnz5m1X1dqBjpWZBJGUlMTcuXMjHYYxxpQqIrI+v2NWxWSMMSYg\nSxDGGGMCsgRhjDEmoDLTBmGMCY8jR46wceNGDh48GOlQzAmIi4ujXr16xMTE+L7GEoQx5oRs3LiR\nxMREkpKSEJFIh2N8UFV27NjBxo0badSoke/rrIrJGHNCDh48SM2aNS05lCIiQs2aNU+41GcJwhhz\nwiw5lD5F+Tcr9wnit9/gwQdh4cJIR2KMMSVLuU8QUVHw0EPKmLcORzoUY4wPO3bsoG3btrRt25ZT\nTjmFunXrHnt++LC//8dDhw5lxYoVBZ7zwgsv8M477wQjZDp37syCBQuCcq9wKveN1NV2rqNr5lom\njm7LP56IjXQ4xphC1KxZ89iX7ahRo0hISOCuu+7KdY6qoqpERQX+G/j1118v9HVuuumm4gdbypX7\nEgQNG9Knyucs2VydtWsjHYwxpqhWrVpFcnIyl112GS1atGDLli0MGzaM1NRUWrRowYMPPnjs3Ky/\n6DMyMqhWrRojRoygTZs2nH322fz6668A3HvvvTz99NPHzh8xYgQdOnTgjDPO4JtvvgFg3759/OEP\nfyA5OZkBAwaQmprqu6Rw4MABhgwZQqtWrUhJSeHLL78EYPHixZx55pm0bduW1q1bs2bNGvbs2UNa\nWhpt2rShZcuWjB07NphvXb7KfQkCEfr0zuCO92DiR5ncekeFSEdkTOlx++0Q7KqTtm3B+2I+UcuX\nL+fNN98kNTUVgEcffZQaNWqQkZFB9+7dGTBgAMnJybmu2bVrF926dePRRx/lzjvv5LXXXmPEiBHH\n3VtVmT17NhMmTODBBx9kypQpPPfcc5xyyil8+OGHLFy4kJSUFN+xPvvss1SsWJHFixezZMkS0tPT\nWblyJS+++CJ33XUXAwcO5NChQ6gq48ePJykpicmTJx+LORysBAGcNjCV5ixlwtu7Ix2KMaYYTjvt\ntGPJAWD06NGkpKSQkpLCsmXLWLp06XHXVKpUibS0NADat2/PunXrAt77kksuOe6cr776ikGDBgHQ\npk0bWrRo4TvWr776issvvxyAFi1aUKdOHVatWkWnTp14+OGHefzxx/n555+Ji4ujdevWTJkyhREj\nRvD1119TtWpV369THFaCAOjRgz5Rr/DkgjvYtQvC9N4bU/oV8S/9UImPjz/2eOXKlTzzzDPMnj2b\natWqcfnllwccBxAbm932WKFCBTIyMgLeu2LFioWeEwxXXHEFZ599Np988gm9e/fmtddeo2vXrsyd\nO5dJkyYxYsQI0tLSuOeee0IWQxYrQQBUqULfNuvJOFqBqVMjHYwxJhh2795NYmIiVapUYcuWLUwN\nwX/uc845h/fffx9wbQeBSij56dKly7FeUsuWLWPLli00adKENWvW0KRJE2677TYuuugiFi1axKZN\nm0hISOCKK67gz3/+M/Pnzw/67xKIlSA8Zw1KotYP25g4pjKXXhpf+AXGmBItJSWF5ORkmjVrRsOG\nDTnnnHOC/hq33HILV155JcnJyce2/Kp/LrjggmPzIHXp0oXXXnuN66+/nlatWhETE8Obb75JbGws\n7777LqNHjyYmJoY6deowatQovvnmG0aMGEFUVBSxsbH8+9//DvrvEoioalheKNRSU1O1WAsGLVnC\nkJZz+Th+IFt3xhFtqdOYgJYtW0bz5s0jHUaJkJGRQUZGBnFxcaxcuZJevXqxcuVKokvoF0igfzsR\nmaeqqYHOL5m/RSQkJ9On1vO8uX0I33wDXbtGOiBjTEm3d+9eevToQUZGBqrKyy+/XGKTQ1GUnd+k\nuETo1aciMa8fZuJHFeja1bq7GmMKVq1aNebNmxfpMELGGqlzqHLxeXRnJhPH2jz3xhhjCSKn886j\nT4XJrPg5np9+inQwxhgTWZYgckpIoM9Z2wCYODHCsRhjTIRZgsij4SXtacUiJn5g1UzGmPLNEkRe\n6en0YSJfzYnl998jHYwxJq/u3bsfN+jt6aef5sYbbyzwuoSEBAA2b97MgAEDAp5z7rnnUlh3+aef\nfpr9+/cfe56ens7OnTv9hF6gUaNG8cQTTxT7PsFkCSKvM86gzylzyTwahTcvljGmBBk8eDBjxozJ\ntW/MmDEMHjzY1/V16tQp1myoeRPEpEmTqFatWpHvV5JZgshLhA7963ISvzLxo8xIR2OMyWPAgAF8\n8sknxxYHWrduHZs3b6ZLly7HxiWkpKTQqlUrxo8ff9z169ato2XLloCbcnvQoEE0b96c/v37c+DA\ngWPn3XjjjcemCr///vsBNwPr5s2b6d69O927dwcgKSmJ7du3A/Dkk0/SsmVLWrZseWyq8HXr1tG8\neXOuu+46WrRoQa9evXK9TmEC3XPfvn1ceOGFx6b/fu+99wAYMWIEycnJtG7d+rg1MorCxkEEEJXe\nm4temsiHk67kyJEKeKPjjTF5RGK27xo1atChQwcmT55Mv379GDNmDJdeeikiQlxcHOPGjaNKlSps\n376ds846i759++a7HvNLL71E5cqVWbZsGYsWLco1XfcjjzxCjRo1yMzMpEePHixatIhbb72VJ598\nkpkzZ1KrVq1c95o3bx6vv/4633//PapKx44d6datG9WrV2flypWMHj2aV199lUsvvZQPP/zw2Eyu\nBcnvnmvWrKFOnTp88skngJv+e8eOHYwbN47ly5cjIkGp9rISRCDdu9Mnegq79sUwa1akgzHG5JWz\nmiln9ZKqcs8999C6dWt69uzJpk2b2Lp1a773+fLLL499Ubdu3ZrWrVsfO/b++++TkpJCu3btWLJk\nSaET8X311Vf079+f+Ph4EhISuOSSS5jlfYE0atSItm3bAgVPKe73nq1atWLatGncfffdzJo1i6pV\nq1K1alXi4uK45ppr+N///kflypV9vUZBrAQRSHw853c9RMWZh5g4sSLnnRfpgIwpmSI123e/fv24\n4447mD9/Pvv376d9+/YAvPPOO2zbto158+YRExNDUlJSwCm+C7N27VqeeOIJ5syZQ/Xq1bnqqquK\ndJ8sWVOFg5su/ESqmAI5/fTTmT9/PpMmTeLee++lR48ejBw5ktmzZzN9+nTGjh3L888/z4wZM4r1\nOlaCyEd83x6cp9OZOO4IZWQ+Q2PKjISEBLp3787VV1+dq3F6165dnHTSScTExDBz5kzWr19f4H26\ndu3Ku+++C8CPP/7IokWLADdVeHx8PFWrVmXr1q3HVnIDSExMZM+ePcfdq0uXLnz00Ufs37+fffv2\nMW7cOLp06VKs3zO/e27evJnKlStz+eWXM3z4cObPn8/evXvZtWsX6enpPPXUUyxcuLBYrw1Wgshf\nWhp9b3+SG9ens3w52OSVxpQsgwcPpn///rl6NF122WX06dOHVq1akZqaSrNmzQq8x4033sjQoUNp\n3rw5zZs3P1YSadOmDe3ataNZs2bUr18/11Thw4YNo3fv3tSpU4eZM2ce25+SksJVV11Fhw4dALj2\n2mtp166d7+okgIcffvhYQzTAxo0bA95z6tSpDB8+nKioKGJiYnjppZfYs2cP/fr14+DBg6gqTz75\npO/XzY9N950fVTY26kL99V/x6KNw993Bu7UxpZlN9116neh034VWMYnI4yJSRURiRGS6iGwTkcKb\n30s7Eer1aUc7WcDE8UcjHY0xxoSdnzaIXqq6G7gIWAc0AYaHMqgSIz2dPjqeb78XvG7OxhhTbvhJ\nEFntFBcCH6jqrhDGU7Kcey59Yj/l6FFh0qRIB2NMyVFWqqbLk6L8m/lJEB+LyHKgPTBdRGoD5WMm\nu0qVSDmvGqdW2GqzuxrjiYuLY8eOHZYkShFVZceOHcTFxZ3QdYX2YlLVESLyOLBLVTNFZB/Qr4hx\nljpR6b3pM+Uj3p18HYcORZGjO7Mx5VK9evXYuHEj27Zti3Qo5gTExcVRr169E7qm0AQhIn8EpnjJ\n4V4gBXgY+KVIUZY2aWn04XZe2Xc9X3wBvXpFOiBjIismJoZGjRpFOgwTBn6qmO5T1T0i0hnoCfwH\neCm0YZUgTZrQ47T1VIo6aNVMxphyxU+CyJrS9ELgFVX9BIj1c3MR6S0iK0RklYiMCHD8ThFZKiKL\nvC60DXMcGyIiK71tiJ/XC5VKF/WgJ58xccJRG1VtjCk3/CSITSLyMjAQmCQiFf1cJyIVgBeANCAZ\nGCwiyXlO+wFIVdXWwFjgce/aGsD9QEegA3C/iFT39yuFQFoafY6OZ/2GKH78MWJRGGNMWPlJEJcC\nU4ELVHUnUAN/4yA6AKtUdY2qHgbGkKdxW1VnqmrWyhvfAVktKBcA01T1N1X9HZgG9PbxmqHRrRsX\nxU0HYMKEiEVhjDFhVWiC8L7AVwMXiMjNwEmq+qmPe9cFfs7xfKO3Lz/XAFkzYvm6VkSGichcEZkb\n0h4VcXGc2iOZMysuZOJEq2MyxpQPfqqKbgPeAU7ytrdF5JZgBuFN3ZEK/PNErlPVV1Q1VVVTa9eu\nHcyQjpeeTp9DY5k9GwqYXt4YY8oMP1VM1wAdVXWkqo4EzgKu83HdJqB+juf1vH25iEhP4G9AX1U9\ndCLXhlVaGn2YiKrgLeJkjDFlmp8EIWT3ZMJ7HHj9vtzmAE1FpJGIxAKDgFw1+CLSDngZlxx+zXFo\nKtBLRKp7jdO9vH2R06gRbc44RP24X627qzGmXPCTIF4HvheRUSIyCteY/FphF6lqBnAz7ot9GfC+\nqi4RkQdFpK932j+BBOADEVkgIhO8a38DHsIlmTnAg96+iJL0NC46PI5PP1WKsbiUMcaUCr7WgxCR\nFKCz93SWqv4Q0qiKIOjrQQTy2WdMOf8J0pjCJ59AenpoX84YY0KtoPUgfK0op6rzgfk5brhBVRsE\nKb7So0sXzq08kPjDB5k4Mc4ShDGmTCvqmtR+2iDKnooVievZmV6xn/Pxx2qjqo0xZVpRE0T5/WpM\nS6PP/vfYuFFYsCDSwRhjTOjkW8UkInfmdwjXsFw+paVxISMRUSZMENq1i3RAxhgTGgWVIBLz2RKA\nZ0IfWgnVsCEnJdfmrCpLrburMaZMy7cEoaoPhDOQUiU9nT5PjuaeeQ+zeTPUqRPpgIwxJviK2gZR\nvqWl0efoRwB8/HGEYzHGmBCxBFEUnTvTIn49SYnbrZrJGFNm+V3XweQUG4uc35M+OpHPPlP27y/8\nEmOMKW38lCBWisg/Ayz2U76lp9Nn77scPCh89lmkgzHGmODzkyDaAD8B/yci33lrMFQJcVwlX1oa\n3fiCxIqHrJrJGFMm+VkwaI+qvqqqnYC7cUuBbhGRN0SkScgjLKnq1SO2VTN6V/mWjz+Go0cjHZAx\nxgSXrzYIEekrIuOAp4F/AY2BicCkEMdXsqWn02fHf/nlF5g3L9LBGGNMcPlqg8CtJf1PVW2nqk+q\n6lZVHQtMCW14JVxaGulHJxIXm8lf/gJHjkQ6IGOMCR4/CaK1ql6jqt/kPaCqt4YgptKjUydqVsng\n1bNf5/PP4bbbIh2QMcYEj58EcZKITBSR7SLyq4iMF5HGIY+sNIiJgfPP5/JVo/jLcOWll+CllyId\nlDHGBIefBPEu8D5wClAH+AAYHcqgSpX0dNi0ib8PXkx6Otx6K3z+eaSDMsaY4vOTICqr6luqmuFt\nbwNxoQ6s1OjdG0SoMOYd3n0XmjaFAQNg7dpIB2aMMcXjJ0FMFpERIpIkIg1F5C/AJBGpISI1Qh1g\niVenDgwaBM8/T9VDvzJhguvy2rcv7NkT6eCMMabo/CSIS4HrgZnA58CNwCBgHhDiRaBLifvvh4MH\n4bHHaNIE3n8fli2DK66w8RHGmNLLz0C5RgVs1lgNcMYZLhu8+CJs3kzPnvDkkzB+vMsdxhhTGvkZ\nKBcjIreKyFhvu1lEYsIRXKkyciRkZMA//gHALbfANdfAww/De+9FODZjjCkCP1VMLwHtgRe9rb23\nz+TUuDEMHQqvvAIbNiDiChSdO7vd8+dHOkBjjDkxfhLEmao6RFVneNtQ4MxQB1Yq3Xuv+/nIIwDE\nxsKHH0KtWtCvH/zySwRjM8aYE+QnQWSKyGlZT7xBcpmhC6kUa9AArrsOXnsN1qwB4KSTYMIE+O03\nuOQSOHQowjEaY4xPfhLEcGCmiHwuIl8AM4A/hzasUuyee6BCBdf44GnbFt54A779Fm64AVQjGJ8x\nxvhUYIIQkSjgANAUuBW4BThDVWeGIbbSqU4duPFGePNNWLny2O4BA1yPpv/+F555JnLhGWOMXwUm\nCFU9CrygqodUdZG3WSVJYUaMgIoV4YEHcu0eOdJVM/35zzB1aoRiM8YYn/xUMU0XkT+IiIQ8mrLi\n5JPh5pvh3Xdh6dJju6OiXFVTy5YwcCD89FMEYzTGmEL4SRDX4yboOyQiu0Vkj4jsDnFcpd/w4RAf\nD6NG5dqdkOAarWNj3XQcO3dGJjxjjCmMn5HUiaoapaqxqlrFe25rUhemVi24/Xb44ANYuDDXoYYN\nXffX1ath8GDItD5hxpgSyM9I6ul+9pkA7rwTqlYNON9Gly5uIN2UKXD33RGIzRhjCpFvghCROG+2\n1loiUj1r9lYRSQLqhivAUq16ddciPX48zD1+XsPrrnNNFf/6l2ubMMaYkqSgEsT1uBlbm3k/s7bx\nwPOhD62MuO02qFHDdWEK4KmnoEcPGDbMjZMwxpiSIt8EoarPqGoj4C5VbZxjBtc2qmoJwq8qVVyD\n9eTJATNAdLSbHrx+fejfH37+OQIxGmNMAKI+hvWKSCcgCYjO2qeqb4YurBOXmpqqcwNU45QIe/e6\nyfzatIFp0wKesnQpnHWWW5Fu1iyoXDnMMRpjyiURmaeqqYGO+Wmkfgt4AuiMm6TvTCDgzQJc21tE\nVojIKhEZEeB4VxGZLyIZIjIgz7FMEVngbRP8vF6JlZDgBs999hl8+WXAU5KTYfRo+OEHN/urTcdh\njIm0QksQIrIMSFY/RY3c11UAfgLOBzYCc4DBqro0xzlJQBXgLmCCqo7NcWyvqib4fb0SXYIAOHAA\nTjvNFRE+/xzyGXf4+OOuV9NDD2VPDmuMMaFSrBIE8CNwShFetwOwSlXXqOphYAzQL+cJqrpOVRcB\nZX9hzkqV3ER+X34J0/PvJTx8uFuc7r77YNy4MMZnjDF5+EkQtYClIjJVRCZkbT6uqwvkbHLdyIl1\nj40Tkbki8p2IXBzoBBEZ5p0zd9u2bSdw6wi57jrXGn3fffnWIYm4NYc6dnSJYtGiMMdojDGe6MJP\nYVSog8hHQ1Xd5K0/MUNEFqvq6pwnqOorwCvgqpgiEeQJqVjR1Rtdf73r1ZSeHvC0uDhXejjzTDcd\nx+zZbl0JY4wJp4IGyjUDUNUvgO9U9YusDfAzo+smoH6O5/W8fb6o6ibv5xrgc6Cd32tLtKFDoVEj\nNy6igGadU0+Fjz6CrVvdVOGHD4cxRmOMoeAqpndzPM7bgf9FH/eeAzQVkUYiEgsMAnz1RvJGblf0\nHtcCzgGWFnxVKRET46qY5s1zI6wLkJrqFqebNcuNuLaeTcaYcCooQUg+jwM9P46qZgA3A1OBZcD7\nqrpERB4Ukb4AInKmiGwE/gi8LCJLvMubA3NFZCEwE3g0Z++nUu+KK1xvppEj4WjB7fODB7u27Vdf\nhedteKIxJowKaoPQfB4Heh74BqqTgEl59o3M8XgOruop73XfAK38vEapFB3tJvC7/HIYOxYuvbTA\n0x96CJYsgTvugObNoWfPMMVpjCnX8h0HISK/4rqmCjDQe4z3/FJVPTksEfpU4sdB5JWZCa1bu3qj\nxYvdOtYF2LMHOnWCjRtdo3XTpmGK0xhTphV1HMRw3OR8c3M8znr+l2AHWe5UqOAWE1q2DMaMKfT0\nxES30FCFCq5n065doQ/RGFO++ZqLqTQodSUIcO0P7dq5UdZLl7qqp0J88YWrYjr/fJg4sdCChzHG\nFKi4I6lNqERFwYMPwsqV8PLLvi7p1s01Vk+e7KZ3MsaYULEEEWl9+7oiwV13wY8/+rrk+uvhppvg\niSdsoSFjTOhYgog0EXjrLbduxMCBsG+fr8ueegrOO88WGjLGhI6f6b4fF5EqIhIjItNFZJuIXB6O\n4MqNU06Bt992Dda33urrkpiY3AsNbdgQ4hiNMeWOnxJEL1XdDVwErAOa4Ho1mWA6/3z461/d0Ol3\n3y38fKBmTdez6cAB6N4d1q8PcYzGmHLFT4LI6lpzIfCBqloHy1B54AE45xzXyLBypa9LkpPh009h\nxw7o2hVWry78GmOM8cNPgvhYRJYD7YHpIlIbOBjasMqp6GhXeoiJce0Rh/zMieimBp8xw61s2q0b\nrFgR4jiNMeVCoQlCVUcAnYBUVT0C7CPPwj8miBo0gP/+1609Otx/TV5Kiluo7vBhlySWLCn0EmOM\nKZCfRuo/AkdUNVNE7gXeBuqEPLLyrG9fuO02eO45N+e3T61auYF0UVFw7rmwcGHoQjTGlH1+qpju\nU9U9ItIZ6An8B3gptGEZHnsM2rd360ecQOtz8+ZuVdNKlVzDdWkbXG6MKTn8JIhM7+eFwCuq+gkQ\nG7qQDOBWn3vvPTep3+DBcOSI70ubNHFJolo16NHDxkkYY4rGT4LYJCIv42Z0neQt5GMD7MLhtNPc\nAtXffuvWjjgBSUmuuunkk10P2i++CE2Ixpiyy88X/aW4RX8uUNWdQA1sHET4DBoE110Hjz4KU6ee\n0KX167vE0KABpKXBZ5+FKEZjTJnkpxfTfmA1cIGI3AycpKqfhjwyk+3pp6FFC7cS3ZYtJ3Tpqae6\n3k1NmsBFF8GkSYVeYowxgL9eTLcB7wAnedvbInJLqAMzOVSu7ObV2LvXrUKXmVn4NTmcdBLMnOly\nzMUXn1DHKGNMOeaniukaoKOqjvSWCz0LuC60YZnjJCe7eb5nzIC///2EL69ZE6ZPd+MlBgxw+cYY\nYwriJ0EI2T2Z8B5LaMIxBRo6FP70J7cS3ZdfnvDl1arBtGlu6dLBg938gMYYkx8/CeJ14HsRGSUi\no4DvcGMhTLiJwL//DY0bu2/47dtP+BaJiW6xoXPPhSuvhP/Yv6QxJh9+GqmfBIYCv3nbUFV9OtSB\nmXwkJrrxEdu3w1VXQRGWjI2Ph48/hgsugGuvhRdfDH6YxpjSr8AEISIVRGS5qs5X1We97YdwBWfy\nkZLilpP75BO3clARVKrkGqv79HGr040cecJt38aYMq7ABKGqmcAKEWkQpniMXzff7Lok3X03zJ5d\npFtUrAhjx8KQIfDQQ9CrF/zyS5DjNMaUWn7aIKoDS7zV5CZkbaEOzBRCxC0uVKeOG0y3c2eRbhMb\nC6+/7toivvkG2rVzXWKNMcbXZH241eQeBP6VYzORVr06jBnj1hu96CLYvbtItxGBq692BZGqVaFn\nT3j4YTh6NMjxGmNKlXwThIg0EZFzVPWLnBuum+vG8IVoCnT22TB6NHz/vZt06fffi3yrVq1gzhxX\nILnvPjc9x7ZtQYzVGFOqFFSCeBoI9CfpLu+YKSn++EfXmLBgAZx3XrG+1RMT3fiIl1928zi1bQuz\nZgUxVmNMqVFQgjhZVRfn3entSwpZRKZo+vWDCRNg+XI3yKEYrc0iMGwYfPedm+Wje3f4xz+sysmY\n8qagBFGtgGOVgh2ICYILLnCz8a1f79Yd3Vi8msC2bWHePPjDH+Cee1wzRxHG5hljSqmCEsRcETlu\nziURuRaYF7qQTLF07+6mBd+yBbp2hXXrinW7KlVcO/gLL7i5nNq1g6+/Dk6oxpiSraAEcTswVEQ+\nF5F/edsXuMn7bgtPeKZIzjnHfZvv3AldusDKlcW6nQj8v//nusHGxrrCyRNPFGkQtzGmFMk3Qajq\nVlXtBDwArPO2B1T1bFW14VQl3ZlnugENhw65ksTSpcW+Zfv2MH++a+4YPtz9/O23IMRqjCmR/MzF\nNFNVn/O2GeEIygRJmzZutSAR92f/ggXFvmXVqq7D1DPPwJQprsrpu++KH6oxpuSxtaXLuuRk11+1\nUiXXPjFnTrFvKQK33gpffQVRUa4W69JL3bRQ333nCi3GmNIvpAlCRHqLyAoRWSUiIwIc7yoi80Uk\nQ0QG5Dk2RERWetuQUMZZ5jVt6taPqF4devQIWitzhw6uymnoUDdO78473bi9KlXcmhN33gkffFDs\nzlTGmAgRDVFLo4hUAH4CzseNvJ4DDFbVpTnOSQKqAHcBE1R1rLe/BjAXSAUU12uqvarmO0w4NTVV\n586dG5LfpczYtMkNpNu0CSZOdCWKINq8Gb791pUivv0W5s7NLk3Uq+eSx1lnuZ8pKW6yQGNMZInI\nPFVNDXQsuoCL9uC+nI87BKiqVinkdTsAq1R1jXe/MUA/4FiCUNV13rG8Q7AuAKap6m/e8WlAb2B0\nIa9pClK3rqtu6tkT0tNh3Djo3Ttot69Tx42Z+MMf3PPDh12zR1bC+PZbV6IA1xsqJSU7YfTo4ZZF\nNcaUHAX1YkpU1SoBtkQfyQGgLvBzjucbvX1+FOdaU5BTTnEN182bu25I48eH7KViY1011K23uumi\n1q1zpYz//Q9uuw2io90CeQMHwsknu+nGX3nF5n8ypqTw3QYhIieJSIOsLZRB+SUiw0RkrojM3Wbf\nKv7VqpU96m3AAHj//bC99KmnQv/+8Pjjbo6nXbtcCWP4cFizBq6/3p3Ts6ebD+rXX8MWmjEmj0IT\nhIj0FZGVwFrgC9x4iMk+7r0JqJ/jeT1vnx++rlXVV1Q1VVVTa9eu7fPWBnAN1tOmufqdwYPd2hIR\nEBsLHTu6uZ5WroQffnBrIH5RZaEAABrdSURBVG3YADfc4JLFeefBSy/ZYkbGhJufEsRDwFnAT6ra\nCOgB+On5PgdoKiKNRCQWGAT4XWhoKtBLRKqLSHWgl7fPBFNiIkye7KYJv+YauP/+iA6PFnHzPz3y\nCKxYAQsXujmgNm92I7nr1HHzEL7wgptJxBgTWn4SxBFV3QFEiUiUqs7E9S4qkKpmADfjvtiXAe+r\n6hIReVBE+gKIyJkishH4I/CyiCzxrv0Nl5jmeNuDWQ3WJsji412PpquvhgcfhKuucq3LESYCrVu7\npVCXLYPFi90aFb/+6lZbrVvXDRB/7jmXQIwxwVdoN1cR+Qy4GPgHUAv4FTjTm4ajxLBursWk6paR\nGznS1en8739u2HQJtHSp6w01diz8+KNLJn/8I/zzn9CgRLSOGVN6FNTN1U+CiAcO4EoblwFVgXe8\nUkWJYQkiSN5801U3NWvmpg6vX7/wayJo+XJ46y03ihvgr3+Fu+5yA8eNMYUrKEH4qWI6CYhV1QxV\nfQN4FUgMZoCmBLnySjfJ0oYNbpBCEOZvCqVmzVybxfLl0KePKwAlJ7sCkM02a0zx+EkQHwA5B7Jl\nevtMWdWjR+6JlqaW/P4BDRrAe++5CWwTE91gvfPPhyVLIh2ZMaWXnwQRrarHWi29x7GhC8mUCK1a\nuQEKp50GF14I//lPpCPy5dxz3fxQzz/vfrZpA7ff7pbGMMacGD8JYltWryMAEekH2MKT5UHdum6S\nv5494dprXTeiUlBvEx0NN90EP/3k1tZ+7jk3X+H//R9kZkY6OmNKDz8J4gbgHhHZICI/A3cD14c2\nLFNiVKniusFec43r5TRkSInoButHrVrw4otuXe3mzeG669ygvG++iXRkxpQOfhYMWq2qZwHJQHNV\n7aSqq0IfmikxYmLg1VfdoIS33oK0tFJVZ9O2rZujcPRoNxr7nHPgiits/IQxhck3QYjI5d7PO0Xk\nTmAYMCzHc1OeiMC998Ibb7hqp86dXU+nUkIEBg1yI7T/9jc3/dTpp8Njj9kCR8bkp6ASRLz3MzGf\nzZRHWd1gf/7ZdYP94YdIR3RC4uNdTdnSpa5pZcQIaNnSTQxo03cYk1uBA+W8RX9uVdWnwhdS0dhA\nuTD78cfsqqYPPgjquhLh9Omn8Oc/u18H3PTkffu6mdBbtHAlD2PKsiIPlFPVTGBwSKIypVvLltnd\nYC+6yI1WK4V1Nb16waJFLkE88kh2TVqrVu5Xu/12N7biyJFIR2pM+PmZauMpIAZ4D9iXtV9V54c2\ntBNjJYgI2bPHTfQ3dqzrS/rss6W2NJFlyxb4+GOYMMHNiH7oEFSr5hbh69fP/XpV/CyZFQa//+6a\nhZo2dcNVjDlRxZ2LaWaA3aqq5wUjuGCxBBFhU6fCLbe4RR369XOTIzVqFOmoim3fPpckxo93SWP7\ndtepq3t3VxXVp09kJgjcvt29xc8/D7t3u33XXANPPw0JCeGPx5ReBSUIVLVMbO3bt1cTYQcPqj76\nqGp8vGpcnOqoUar790c6qqDJyFCdNUt1+HDV009XdaMGVdu1U/3731U3bAh9DL/84l4/Pl5VRHXA\nANU5c1Tvucc9b9pUdfbs0Mdhyg5grubzvVroFy9u9tYngbne9i+gamHXhXuzBFGC/Pyz6sCB7uPV\nqJHq+PGqR49GOqqgW75c9fHHVTt1cr+qiGqPHqpvvKG6Z09wX2vjRtXbblOtVEk1Kkr1T39SXbIk\n9zmff65av75qdLRLWBkZwY3BlE3FTRAfAg8Ajb3tfuB/hV0X7s0SRAk0Y4ZqcrL7mKWlqf70U6Qj\nCpnVq12BqXFj9+vGx6teeaXqZ5+pZmYW/b7r16veeKNqbKxqhQqqV12lumJF/uf/9pvqpZe6GLp2\nddcbU5DiJogFfvZFerMEUUIdPqz65JOqiYnuW+6ee1T37o10VCFz9KirhrruOtUqVdz/sPr1Vf/6\nV9Vly/zfZ/Vq1WuvVY2JcduwYapr1viP4Y03VBMSVKtVU33vvaL9LqZ8KG6C+BbonOP5OcC3hV0X\n7s0SRAm3ZYvqFVe4j1y9eqrvv18mq51y2r9fdcwY1fR099c/qHbooPr886rbtwe+ZvlyV/KoUEG1\nYkXVm24qetvGqlWqZ53lXnfIENXdu4v8q5gyrLgJoi2wEFgHrAd+ANoUdl24N0sQpcSsWapt2riP\nXo8ex1ekl1Fbtqj+61+qrVu7Xz0mRrV/f9Vx41QPHVL98UfVQYNcO0alSqp33KG6aVPxX/fwYdWR\nI127RePGqt9+W/x7mrKlWAni2IlQBaji9/xwb5YgSpEjR1RfeMHVf0RHq/75z6o7d0Y6qrBZsED1\nzjtVTz7Z/Q+sWtX9TEhQvftu1a1bg/+aX32lmpTkSiYPPOD+CYxRLThB+BkHEWhivl3APFUtMetR\n2jiIUmjbNrjnHrcYUWIi3HijG7p8yimRjiwsMjLcVB/vvw8NG8Ktt0LNmqF7vV273DoZ77zjZrR9\n+21ISgrd65nSobgD5d4FUoGJ3q6LgEVAEvCBqj4evFCLzhJEKfbDD/Doo240dkwMXHUV3HUXNGkS\n6cjKpHfegf/3/9zjF1+Eyy6LbDwmsoqbIL4E0lV1r/c8AfgE6I0rRSQHOd4isQRRBqxaBU88Aa+/\n7v68HjAA7r4bUlIiHVmZs24dXH45fP01/OlPrmSxfz/s3Rt427Mn/2N797rFB7t1c1vXrnDSSZH+\nDY1fxU0Qy4FWqnrEe14RWKiqzUTkB1VtF/SIi8ASRBmyZQs88wy89JKbR6JXLzcv97nn2vSqQZSR\nAf/4BzzwQMFLscbHu+k78tvi411u//prNzUJuBX8shJGt25w6qnh+Z3MiStugrgP6A+M93b1ASbg\nRlS/oqolooBqCaIM2rUL/v1vN+nQ1q1uLu6774aLL4YoP6vlGj+WL4e1a7O/8BMTsx9Xruz/rT5y\nxC3v+sUXbvvqK1fyADeZYM6EUb9+6H4fc2KKlSC8G6Tixj8AfK2qJe6b2BJEGXbwoJuy9PHHYc0a\nOOMM+MtfXOV5xYqRjs7kIyMDFizIThizZmWvVNu4ce6EUa+eq+Lavx8OHMj9M7/HOfc1bOgmTmzW\nLLyFzKNHXRPa4cNuadtKlcL32sESjATRGWiqqq+LSG0gQVXXBjnOYrEEUQ5kZMCHH7p1Qn/4AerU\ngTvvhGHD3J+9pkTLzITFi12y+Pxzt3Ltb78V756VKkFcnJv2HFy/hj593Na5s+vzEGzbt7veZ5Mn\nu0mMt21z+6OjoU0b6NjRbR06uGVtw1HYPXq06K9T3Cqm+3G9mM5Q1dNFpA6u99I5BV4YZpYgyhFV\nNwf3o4+61XyqVYPrr4cbbrB+m6XI0aOwZEl2oqhc2W2VKh3/ONC+uLjs0sLGjW469okTYfr07DU8\n0tLctOy9e7vnRY1z7lyXECZPhtmz3UewVi244AL3GgkJ8P33bpszJ7tqrVo1OPPM3EmjKA34mZmw\naZPrXLB27fHb6ae737soipsgFgDtgPlZDdIiskhVWxctnNCwBFFOzZ7tqp7GjXPPL7oIbr4ZevSw\ndopyau9e+Owzt+DTxx+7v/Cjo13vqqzSxWmnFXyP7dtd6SCrlLB9u0tGHTq4hJCWBqmpgT9imZmu\nXScrYcye7UpOWR0BkpKyE0bHjtCunUt227YF/vJfuxY2bMi9qqGI6znWqJHb2rd342iKorgJYraq\ndhCR+aqaIiLxuLmYLEGYkmPDBnj5ZXj1Vfc/7fTTXd/NIUOgatVIR2ciJDPTfUFPnOi2rLXHk5Nd\noujb131JQ+5Swpw5rpRQu7YrffTu7TrT1apVtDj27YP587OTxvffw88/u2PR0a4pbd++3NfUrp2d\nAJKSsh83auQWqQpW81txE8RdQFPgfOAfwNXAaFV9NjjhBYclCAO4uoUPPoAXXnBrZsfHwxVXuGTR\nsmWkozMRtnatSxQTJri2kIwM90V89Cjs2OFKBB07ZpcSUlJCVxDdsiW7hHHgQO4EkJQUvpUBg9FI\nfT7QCxBgqqpOC26IxWcJwhxn7lyXKEaPdomjWzdX/dSvX2haL02psmsXTJkCn3ziqmzS0uD880M7\n3UlJVNwSxGOqendh+yLNEoTJ1/bt8NprbuDdunWu8vb66+G668rNvE/G5KegBOGn8HR+gH1pxQvJ\nmDCqVcuNm1i1ytUttGgBI0e6itzLLoNvvnEVzsaYXPJNECJyo4gsBs4QkUU5trW4yfqMKV0qVHAt\nk1OnwooVbsa6jz92U5smJ8OoUbBsWaSjNKbEyLeKSUSqAtVxDdMjchzao6rFHN4SfFbFZIpk717X\nRvHuu67VUhVat4aBA91WWH9IY0q5IlUxqeouVV2nqoNVdT1wAFAgQUQa+Hzh3iKyQkRWiciIAMcr\nish73vHvRSTJ258kIgdEZIG3/dvP6xlzwhISXFvEzJlutNUzz7h9f/ubG5Z75pluhtkNGyIdqTFh\nV2gbhIj0EZGVwFrgC9zSo5N9XFcBeAHXXpEMDBaRvFODXwP8rqpNgKeAx3IcW62qbb3tBj+/jDHF\nUqeOG2309dewfj38859u//DhbrKfTp3g2Wdd/0RjygE/jdQPA2cBP6lqI6AH8J2P6zoAq1R1jaoe\nBsYA/fKc0w94w3s8FughYvM5mxKgQQO3aNGcObByJTzyiBvJdNttrhfUuee6mWazJuIxpgzykyCO\nqOoOIEpEolR1Jm5upsLUBX7O8Xyjty/gOaqagVvKNKsXciMR+UFEvhCRLoFeQESGichcEZm7zf6j\nmlBp0sQtjbpwISxd6npAbd3qlkg99VQ3xPaVV9w+Y8oQPwlip7eK3JfAOyLyDLCvkGuKawvQwJv7\n6U7gXRGpkvckVX1FVVNVNbV27dohDskY3Eo4o0a5RLFwoVufYvVqN67i1FNdj6gnnnBdao0p5fwk\niH7AfuAOYAqwGrdoUGE2ATmXBann7Qt4johEA1WBHap6yCu1oKrzvNc83cdrGhMeIq630yOPuGSw\ncKFLHAcOuDaLpk3d1B733edW0bFxFqYUKmgcRBMROUdV96nqUVXNUNU3gPmAn4lz5wBNRaSRiMQC\ng3Ar0eU0ARjiPR4AzFBVFZHaXiM3ItIYNxfUmhP71YwJk6xkMXKkm5Ft7Vp4+mk3yc/f/+6m/WzY\n0DWAz5iRe1pOY0qwgkoQTwO7A+zf5R0rkNemcDMwFVgGvK+qS0TkQRHp6532H6CmiKzCVSVldYXt\nCizyphofC9xQEsdeGBNQUpJrzJ4507VLvP66m9P51VfdNOQnn+xmmR037vgpPI0pQQoaKDdHVc/M\n59hiVW0V0shOkA2UMyXevn1uKbJx49wI7t9/d6vf9OrlEkdqqluSrHLlSEdqypGCBspFF3BdQdVI\npXDlVWMiLD4e+vd325EjbpHmjz5y2/jx7pwKFdxcUe3bu4SRmuqqr+LiIhu7KZcKKkGMxrUJvJpn\n/7XA+ao6MAzx+WYlCFNqqbr1JOfNc1OUZ23bt7vj0dHQqlV2wmjf3j2PjY1s3KZMKNJ03yJyMjAO\nOAzM83anArFAf1X9JQSxFpklCFOmqLrpPebOzZ04fv/dHY+NdSWLrKTRpYvrOWXjTM0JKu56EN2B\nrKW4lqjqjCDHFxSWIEyZp+p6SOUsZcybB7u9viSNGmWvj9m9OyQmRjZeUyoUe0W50sAShCmXjh51\nU4HMmOGWR5s+3TWGx8RA587ZCaNVKytdmIAsQRhTXhw+7CYbnDLFbYu8pVtOPTU7WfTsCTVqRDZO\nU2JYgjCmvNq82S2QNGWK62K7cydERUHHjtkJo31713vKlEuWIIwxkJHhZqfNKl3MmePaNWrWdFOZ\nt2yZvZ1xBlSsGOmITRhYgjDGHG/7dpg2zZUw5s2D5ctdEgFXojj99NxJo2VLt8KelTbKFEsQxpjC\nHT7sGrx//DH3tnp19mSDFSu6GW3zJo4GDawRvJSyBGGMKbr9+2HZMliyJHfi+DnHci/Vqrn5plJS\nsn+efrqVNkqBok61YYwxbm6o9u3dltPOnW5djMWL3XTn8+fDCy/AwYPZ17VtmztpJCfbCPBSxEoQ\nxpjgychwbRnz57vthx/ctmePOx4b68Zk5EwarVu7SQtNRFgVkzEmco4ede0YWUkja/vNm8G/QgU3\nRXrW1rBh7sd161pVVQhZFZMxJnKiotw8UU2bwkBvjk9V14aRlSxWroR16+CTT+CXPNO8RUdD/frZ\niSNvAqlXz40cN0FnCcIYE34irudTgwZw8cW5jx086CYqXLcO1q93P7MeT5vmBv/lrPmIinIJpHFj\ntzVqlP24cWOoVct6WBWRJQhjTMkSF+d6QJ2ezzL0hw+70kdW4sja1qwJXAJJSMidMHJuDRvaWhsF\nsARhjCldYmPdgL3TTgt8fN++7ISRta1d66qxpk6FAweyzxWBOnWy2zrq1nXPsx5nbeW0Ed0ShDGm\nbImPd6vytWhx/DFVt054zuSxZo0rkSxaBJMnw969x19XvfrxSSNnImnUyJ1TxliCMMaUHyJwyilu\n69Qp8Dm7d7sV/vLbFi1ySebo0dzX1aoFzZq5eaxybo0bl9pGdEsQxhiTU5UqbmvePP9zMjJckshK\nGmvWuPEfK1bAxInwn/9knxsd7ZLEGWccn0BKeAO6JQhjjDlR0dHZ1UuB7NzpksWKFdmJY8UK1wZy\n+HD2edWru0TRpInriZXVsyvrcdWq4fl98mEJwhhjgq1aNbfmRseOufdnZrruunmTx6xZsHGjO55T\nYuLxSSPn47p1QzotuyUIY4wJlwoVsrvYpqXlPpaZ6brobtjgGs1z/tywwa1Bvm3b8fc85RTo1g3G\njAl6uJYgjDGmJKhQIbva6uyzA59z4IAraeRNIrVrhyQkSxDGGFNaVKqUPW1JGESF5VWMMcaUOpYg\njDHGBGQJwhhjTECWIIwxxgRkCcIYY0xAliCMMcYEZAnCGGNMQJYgjDHGBCSac+m+UkxEtgHrIx1H\nAWoB2yMdRAEsvuKx+IrH4iue4sTXUFUDDsUuMwmipBORuaqaGuk48mPxFY/FVzwWX/GEKj6rYjLG\nGBOQJQhjjDEBWYIIn1ciHUAhLL7isfiKx+IrnpDEZ20QxhhjArIShDHGmIAsQRhjjAnIEkSQiEh9\nEZkpIktFZImI3BbgnHNFZJeILPC2kRGIc52ILPZef26A4yIiz4rIKhFZJCIpYYztjBzvzQIR2S0i\nt+c5J6zvoYi8JiK/isiPOfbVEJFpIrLS+1k9n2uHeOesFJEhYYzvnyKy3Pv3Gyci1fK5tsDPQgjj\nGyUim3L8G6bnc21vEVnhfRZHhDG+93LEtk5EFuRzbTjev4DfK2H7DKqqbUHYgFOBFO9xIvATkJzn\nnHOBjyMc5zqgVgHH04HJgABnAd9HKM4KwC+4QTwRew+BrkAK8GOOfY8DI7zHI4DHAlxXA1jj/azu\nPa4epvh6AdHe48cCxefnsxDC+EYBd/n4918NNAZigYV5/z+FKr48x/8FjIzg+xfweyVcn0ErQQSJ\nqm5R1fne4z3AMqBuZKMqkn7Am+p8B1QTkVMjEEcPYLWqRnR0vKp+CfyWZ3c/4A3v8RvAxQEuvQCY\npqq/qervwDSgdzjiU9VPVTXDe/odUC/Yr+tXPu+fHx2AVaq6RlUPA2Nw73tQFRSfiAhwKTA62K/r\nVwHfK2H5DFqCCAERSQLaAd8HOHy2iCwUkcki0iKsgTkKfCoi80RkWIDjdYGfczzfSGQS3SDy/48Z\n6ffwZFXd4j3+BTg5wDkl5X28GlciDKSwz0Io3exVgb2WT/VISXj/ugBbVXVlPsfD+v7l+V4Jy2fQ\nEkSQiUgC8CFwu6ruznN4Pq7KpA3wHPBRuOMDOqtqCpAG3CQiXSMQQ4FEJBboC3wQ4HBJeA+PUVeW\nL5F9xUXkb0AG8E4+p0Tqs/AScBrQFtiCq8YpiQZTcOkhbO9fQd8rofwMWoIIIhGJwf0jvqOq/8t7\nXFV3q+pe7/EkIEZEaoUzRlXd5P38FRiHK8rntAmon+N5PW9fOKUB81V1a94DJeE9BLZmVbt5P38N\ncE5E30cRuQq4CLjM+wI5jo/PQkio6lZVzVTVo8Cr+bxupN+/aOAS4L38zgnX+5fP90pYPoOWIILE\nq6/8D7BMVZ/M55xTvPMQkQ64939HGGOMF5HErMe4xswf85w2AbjS6810FrArR1E2XPL9yy3S76Fn\nApDVI2QIMD7AOVOBXiJS3atC6eXtCzkR6Q38BeirqvvzOcfPZyFU8eVs0+qfz+vOAZqKSCOvRDkI\n976HS09guapuDHQwXO9fAd8r4fkMhrIFvjxtQGdcMW8RsMDb0oEbgBu8c24GluB6ZHwHdApzjI29\n117oxfE3b3/OGAV4AdeDZDGQGuYY43Ff+FVz7IvYe4hLVFuAI7g63GuAmsB0YCXwGVDDOzcV+L8c\n114NrPK2oWGMbxWu7jnrc/hv79w6wKSCPgthiu8t77O1CPdFd2re+Lzn6bheO6vDGZ+3/79Zn7kc\n50bi/cvveyUsn0GbasMYY0xAVsVkjDEmIEsQxhhjArIEYYwxJiBLEMYYYwKyBGGMMSYgSxDGFEJE\nMiX3LLNBm1lURJJyziRqTEkSHekAjCkFDqhq20gHYUy4WQnCmCLy1gN43FsTYLaINPH2J4nIDG8y\nuuki0sDbf7K49RkWelsn71YVRORVb77/T0Wkknf+rd46AItEZEyEfk1TjlmCMKZwlfJUMQ3McWyX\nqrYCngee9vY9B7yhqq1xE+U96+1/FvhC3USDKbgRuABNgRdUtQWwE/iDt38E0M67zw2h+uWMyY+N\npDamECKyV1UTAuxfB5ynqmu8CdV+UdWaIrIdN33EEW//FlWtJSLbgHqqeijHPZJwc/Y39Z7fDcSo\n6sMiMgXYi5ux9iP1Jik0JlysBGFM8Wg+j0/EoRyPM8luG7wQNy9WCjDHm2HUmLCxBGFM8QzM8fNb\n7/E3uNlHAS4DZnmPpwM3AohIBRGpmt9NRSQKqK+qM4G7garAcaUYY0LJ/iIxpnCVJPfC9VNUNaur\na3URWYQrBQz29t0CvC4iw4FtwFBv/23AKyJyDa6kcCNuJtFAKgBve0lEgGdVdWfQfiNjfLA2CGOK\nyGuDSFXV7ZGOxZhQsComY4wxAVkJwhhjTEBWgjDGGBOQJQhjjDEBWYIwxhgTkCUIY4wxAVmCMMYY\nE9D/B2QIQU6zxHqxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loss :  0.01690642379925382\n",
            "\n",
            "Validation Loss :  0.0629027117189508\n",
            "\n",
            "Training Accuracy :  0.996986531986532\n",
            "\n",
            "Validation Accuracy :  0.9883333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BL0a_jRrmMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict labels for test set.\n",
        "\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBh66Utetu9e",
        "colab_type": "code",
        "outputId": "a422ba44-7b9a-4537-c99c-7fd9495cc9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate Model Accuracy.\n",
        "\n",
        "print(model.accuracy(Y_pred,Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysV7czsasUB_",
        "colab_type": "text"
      },
      "source": [
        "#**REGRESSION on Boston Housing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HLbZcDJVcr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import packages\n",
        "\n",
        "from keras.datasets import boston_housing\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs73ttj7tcxu",
        "colab_type": "code",
        "outputId": "ef824312-9322-43c0-d6a5-038d2b977435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load Boston Housing dataset \n",
        "\n",
        "(X, Y), (X_test, Y_test) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je2XfoKSCYfl",
        "colab_type": "code",
        "outputId": "cc65cd54-f467-43cc-ddb9-4a7688f16909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# Compute Correlation Matrix\n",
        "\n",
        "data = pd.DataFrame(np.concatenate((X,Y.reshape(-1,1)),axis=1))\n",
        "print(data.corr())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          0         1         2   ...        11        12        13\n",
            "0   1.000000 -0.192179  0.397419  ... -0.390613  0.434384 -0.378498\n",
            "1  -0.192179  1.000000 -0.533823  ...  0.176006 -0.415237  0.380299\n",
            "2   0.397419 -0.533823  1.000000  ... -0.372885  0.603129 -0.476743\n",
            "3  -0.050828 -0.041981  0.052839  ...  0.037832 -0.011017  0.168661\n",
            "4   0.405765 -0.521713  0.774200  ... -0.409479  0.592994 -0.438328\n",
            "5  -0.217597  0.338683 -0.409924  ...  0.145525 -0.610844  0.681483\n",
            "6   0.344410 -0.578728  0.656350  ... -0.278403  0.590898 -0.364173\n",
            "7  -0.378590  0.650787 -0.725155  ...  0.295995 -0.507075  0.253900\n",
            "8   0.609689 -0.311091  0.599226  ... -0.478245  0.490250 -0.375515\n",
            "9   0.575652 -0.303522  0.701362  ... -0.471777  0.534752 -0.448737\n",
            "10  0.273447 -0.403139  0.379284  ... -0.178060  0.365873 -0.493990\n",
            "11 -0.390613  0.176006 -0.372885  ...  1.000000 -0.376081  0.343953\n",
            "12  0.434384 -0.415237  0.603129  ... -0.376081  1.000000 -0.730793\n",
            "13 -0.378498  0.380299 -0.476743  ...  0.343953 -0.730793  1.000000\n",
            "\n",
            "[14 rows x 14 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sspQEh31G4hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop unecessary features\n",
        "\n",
        "X = X[:,[5,12]]\n",
        "X_test = X_test[:,[5,12]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buBHUTjXr7ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize, reshape and split into train, test and validation datasets.\n",
        "\n",
        "X_train,X_val,Y_train,Y_val = train_test_split(X,Y,test_size=0.1)\n",
        "\n",
        "Y_train = Y_train.reshape(1,Y_train.shape[0])\n",
        "Y_val = Y_val.reshape(1,Y_val.shape[0])\n",
        "\n",
        "X_train = (X_train-np.mean(X_train,axis=0))/np.std(X_train,axis=0)\n",
        "X_val = (X_val-np.mean(X_val,axis=0))/np.std(X_val,axis=0)\n",
        "X_test = (X_test-np.mean(X_test,axis=0))/np.std(X_test,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlL1ndJdxutV",
        "colab_type": "code",
        "outputId": "c5ba775e-91bc-4f44-de3c-724e295b8175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "# Define and train the Deep Neural Network.\n",
        "\n",
        "model = Deep_Neural_Network()\n",
        "\n",
        "model.create(2,1,[10,20],output_type='regression',activation='relu',initializer='he')\n",
        "\n",
        "costs = model.train(X_train.T,Y_train,X_val.T,Y_val,optimizer='rmsprop',regularizer='l2',\n",
        "                    regularizer_lambda=0.1,mini_batch_size=32,epochs=50,print_loss_freq=10,\n",
        "                    learning_rate=0.002)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 10 epochs :    Training Loss = 74.00527070049927    Validation Loss = 37.72745585699798\n",
            "\n",
            "After 20 epochs :    Training Loss = 18.262741666185946    Validation Loss = 26.508684928087593\n",
            "\n",
            "After 30 epochs :    Training Loss = 12.317585195713873    Validation Loss = 17.933010242716158\n",
            "\n",
            "After 40 epochs :    Training Loss = 10.364297647267557    Validation Loss = 15.580683447759387\n",
            "\n",
            "After 50 epochs :    Training Loss = 9.79475027726448    Validation Loss = 13.860932504828817\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3xUVf7/8dcnIfQSSgApSlU6GCLq\nIio2BAsiKB0b4rpYUMRl/W5xXd2fbbGvBcUVRbAAoiugLLIiqysGVBRRwyJKb0qvIef3x5lJBkgg\n5U5mkryfj8d93JmbmXPPDWE+c8r9HHPOISIiApAQ6wqIiEj8UFAQEZFsCgoiIpJNQUFERLIpKIiI\nSLZysa5AUdSpU8c1adIk1tUQESlRFi1atNk5l5Lbz0p0UGjSpAnp6emxroaISIliZj/m9TN1H4mI\nSDYFBRERyaagICIi2Ur0mIKIFI8DBw6wevVq9u7dG+uqSAFUrFiRRo0akZSUlO/3KCiIyDGtXr2a\natWq0aRJE8ws1tWRfHDOsWXLFlavXk3Tpk3z/T51H4nIMe3du5fatWsrIJQgZkbt2rUL3LpTUBCR\nfFFAKHkK829WNoPCt9/CqFGwf3+sayIiElfKZlBYsQIeewzeeSfWNRGRY9iyZQudOnWiU6dO1K9f\nn4YNG2Y/35/PL3bXXHMN33333VFf89RTTzFp0qQgqswZZ5zBF198EUhZxa1sDjT36AGNG8Nzz0Hf\nvrGujYgcRe3atbM/YO+++26qVq3KHXfccchrnHM450hIyP177osvvnjM84wcObLolS0FymZLITER\nrr0W5syBlStjXRsRKYTly5fTpk0bBg8eTNu2bVm3bh0jRowgLS2Ntm3bcs8992S/NvzNPTMzk+Tk\nZMaOHUvHjh05/fTT2bhxIwC///3vefTRR7NfP3bsWLp06cJJJ53Exx9/DMCuXbvo27cvbdq0oV+/\nfqSlpeW7RbBnzx6uuuoq2rdvT2pqKvPnzwfgq6++4pRTTqFTp0506NCBFStWsGPHDnr27EnHjh1p\n164db775ZpC/uqMqmy0F8EHhnnvghRfgL3+JdW1ESo5RoyDorpFOnSD0gVwQ3377LRMnTiQtLQ2A\n+++/n1q1apGZmUn37t3p168fbdq0OeQ927Zt46yzzuL+++/n9ttvZ8KECYwdO/aIsp1zLFy4kLff\nfpt77rmH2bNn88QTT1C/fn2mTp3Kl19+SWpqar7r+vjjj1OhQgW++uorli5dSq9evcjIyODvf/87\nd9xxB/3792ffvn0455gxYwZNmjRh1qxZ2XUuLmWzpQBw/PFw4YUwYQJkZsa6NiJSCM2bN88OCACT\nJ08mNTWV1NRUli1bxjfffHPEeypVqkTPnj0B6Ny5Myvz6C24/PLLj3jNggULGDBgAAAdO3akbdu2\n+a7rggULGDJkCABt27alQYMGLF++nF/96lfce++9PPjgg6xatYqKFSvSoUMHZs+ezdixY/nPf/5D\njRo18n2eoiq7LQWAESOgTx+YNQsuuSTWtREpGQrxjT5aqlSpkv04IyODxx57jIULF5KcnMyQIUNy\nnaNfvnz57MeJiYlk5vGlsEKFCsd8TRCGDh3K6aefzrvvvsuFF17IhAkTOPPMM0lPT2fmzJmMHTuW\nnj17ctddd0WtDpHKbksB4KKLoH59GD8+1jURkSLavn071apVo3r16qxbt4733nsv8HN07dqV119/\nHfBjAbm1RPLSrVu37NlNy5YtY926dbRo0YIVK1bQokULbr31Vi6++GKWLFnCmjVrqFq1KkOHDmX0\n6NEsXrw48GvJS9luKSQlwTXXwAMPwJo10LBhrGskIoWUmppKmzZtaNWqFSeccAJdu3YN/Bw333wz\nw4YNo02bNtlbXl07PXr0yM451K1bNyZMmMANN9xA+/btSUpKYuLEiZQvX55XX32VyZMnk5SURIMG\nDbj77rv5+OOPGTt2LAkJCZQvX55nnnkm8GvJiznniu1kQUtLS3NFXmTnf/+DFi38oPMf/hBMxURK\nmWXLltG6detYVyPmMjMzyczMpGLFimRkZHDBBReQkZFBuXLx+/06t387M1vknEvL7fVR6z4ys8Zm\nNs/MvjGzpWZ2a+j43Wa2xsy+CG29It7zOzNbbmbfmVmPaNXtEM2bw7nn+llIWVnFckoRKZl27txJ\n165d6dixI3379uXZZ5+N64BQGNG8mkxgtHNusZlVAxaZ2ZzQzx5xzj0c+WIzawMMANoCDYB/mdmJ\nzrmDUayjd/31MGCAv2+hR/HEIhEpeZKTk1m0aFGsqxFVUWspOOfWOecWhx7vAJYBR+u07w1Mcc7t\nc879ACwHukSrfoe47DKoXVsDziJS5hXL7CMzawKcDHwaOnSTmS0xswlmVjN0rCGwKuJtq8kliJjZ\nCDNLN7P0TZs2BVPBChXg6qthxgzYsCGYMkVESqCoBwUzqwpMBUY557YDTwPNgU7AOuBvBSnPOfec\ncy7NOZeWkpISXEWHD/c3sb30UnBlioiUMFENCmaWhA8Ik5xz0wCccxuccwedc1nAeHK6iNYAjSPe\n3ih0rHi0agXduvkupBI8I0tEpCiiOfvIgBeAZc65cRHHj4t4WR/g69Djt4EBZlbBzJoCLYGF0apf\nrq6/HpYvh3//u1hPKyJH17179yNuRnv00Ue58cYbj/q+qlWrArB27Vr69euX62vOPvtsjjW1/dFH\nH2X37t3Zz3v16sXWrVvzU/Wjuvvuu3n44YeP/cJiFM2WQldgKHDOYdNPHzSzr8xsCdAduA3AObcU\neB34BpgNjCyWmUeR+vWD5GR4/vliPa2IHN3AgQOZMmXKIcemTJnCwIED8/X+Bg0aFCnT6OFBYebM\nmSQnJxe6vHgWzdlHC5xz5pzr4JzrFNpmOueGOufah45f6pxbF/Ge+5xzzZ1zJznnZkWrbnmqVAmG\nDIGpU+Hnn4v99CKSu379+vHuu+9mL6qzcuVK1q5dS7du3di5cyfnnnsuqamptG/fnhkzZhzx/pUr\nV9KuXTvAp7AeMGAArVu3pk+fPuzZsyf7dTfeeGN26u0//elPgM9uunbtWrp370737t0BaNKkCZs3\nbwZg3LhxtGvXjnbt2mWn3l65ciWtW7fm+uuvp23btlxwwQWHnOdYcitz165dXHTRRdnptF977TUA\nxo4dS5s2bejQocMR60wURum66yIIw4fDk0/CK6/ALbfEujYicScWmbNr1apFly5dmDVrFr1792bK\nlClceeWVmBkVK1Zk+vTpVK9enc2bN3Paaadx6aWX5rk+8dNPP03lypVZtmwZS5YsOST99X333Uet\nWrU4ePAg5557LkuWLOGWW25h3LhxzJs3jzp16hxS1qJFi3jxxRf59NNPcc5x6qmnctZZZ1GzZk0y\nMjKYPHky48eP58orr2Tq1KnZWVKPJq8yV6xYQYMGDXj33XcBn057y5YtTJ8+nW+//RYzC6RLq2wn\nxMtNx45wyikacBaJM5FdSJFdR8457rrrLjp06MB5553HmjVr2HCUqeXz58/P/nDu0KEDHTp0yP7Z\n66+/TmpqKieffDJLly49ZsK7BQsW0KdPH6pUqULVqlW5/PLL+eijjwBo2rQpnTp1Ao6eoju/ZbZv\n3545c+bw29/+lo8++ogaNWpQo0YNKlasyHXXXce0adOoXLlyvs5xNGop5Gb4cLjhBli4EE49Nda1\nEYkrscqc3bt3b2677TYWL17M7t276dy5MwCTJk1i06ZNLFq0iKSkJJo0aZJryuxj+eGHH3j44Yf5\n7LPPqFmzJldffXWhygkLp94Gn367IN1HuTnxxBNZvHgxM2fO5Pe//z3nnnsuf/zjH1m4cCFz587l\nzTff5Mknn+SDDz4o0nnUUsjNwIFQpYoGnEXiSNWqVenevTvXXnvtIQPM27Zto27duiQlJTFv3jx+\n/PHHo5Zz5pln8uqrrwLw9ddfs2TJEsCn3q5SpQo1atRgw4YN2aueAVSrVo0dO3YcUVa3bt146623\n2L17N7t27WL69Ol069atSNeZV5lr166lcuXKDBkyhDFjxrB48WJ27tzJtm3b6NWrF4888ghffvll\nkc4Nainkrlo16N8fJk+GceP8cxGJuYEDB9KnT59DZiINHjyYSy65hPbt25OWlkarVq2OWsaNN97I\nNddcQ+vWrWndunV2i6Njx46cfPLJtGrVisaNGx+SenvEiBFceOGFNGjQgHnz5mUfT01N5eqrr6ZL\nF3+71fDhwzn55JPz3VUEcO+992YPJgOsXr061zLfe+89xowZQ0JCAklJSTz99NPs2LGD3r17s3fv\nXpxzjBs3Lq/T5JtSZ+flv/+F00/3YwvDh0fnHCIlhFJnl1xxkzq7xDv1VGjbVknyRKRMUVDIi5m/\nw3nhQgj1OYqIlHYKCkczZAiUL68BZxH81E8pWQrzb6agcDS1a0PfvvDyy1DE6WQiJVnFihXZsmWL\nAkMJ4pxjy5YtVKxYsUDv0+yjYxk+3M9CmjYNBg+OdW1EYqJRo0asXr2awNYwkWJRsWJFGjVqVKD3\naPbRsWRlwYknQuPGEDEVTUSkpNLso6JISIDrrvPptDMyYl0bEZGoUlDIj6uvhsREeOGFWNdERCSq\nFBTy47jjoGdPnzn1YPEu8SAiUpwUFPJr2DBYs0bjCiJSqiko5Ncll0CNGjBxYqxrIiISNQoK+VWx\nok+SN3Uq7NwZ69qIiESFgkJBDBsGu3f7exZEREohBYWC+NWvoFkzdSGJSKmloFAQZjB0KHzwAaxe\nHevaiIgETkGhoIYO9Ws3T5oU65qIiAROQaGgmjeHrl19F1IJThEiIpIbBYXCGDYMvvkGFi+OdU1E\nRAKloFAYV1wBFSpowFlESh0FhcKoWRMuvRRefRUOHIh1bUREAqOgUFjDhsHmzTB7dqxrIiISGAWF\nwurRA1JS1IUkIqWKgkJhJSXBwIHwzjvwyy+xro2ISCAUFIpi2DDYtw/eeCPWNRERCUTUgoKZNTaz\neWb2jZktNbNbQ8drmdkcM8sI7WuGjpuZPW5my81siZmlRqtugUlNhTZt4KWXYl0TEZFARLOlkAmM\nds61AU4DRppZG2AsMNc51xKYG3oO0BNoGdpGAE9HsW7BCKe9+PhjWLEi1rURESmyqAUF59w659zi\n0OMdwDKgIdAbCH+1fgm4LPS4NzDRef8Fks3suGjVLzCDB/vg8Morsa6JiEiRFcuYgpk1AU4GPgXq\nOefWhX60HqgXetwQWBXxttWhY4eXNcLM0s0sfdOmTVGrc741bgxnnw0vv6y0FyJS4kU9KJhZVWAq\nMMo5tz3yZ845BxTok9Q595xzLs05l5aSkhJgTYtg6FBYvhw+/TTWNRERKZKoBgUzS8IHhEnOufDK\nNBvC3UKh/cbQ8TVA44i3Nwodi399+/qV2V5+OdY1EREpkmjOPjLgBWCZc25cxI/eBq4KPb4KmBFx\nfFhoFtJpwLaIbqb4Vr06XHYZTJkC+/fHujYiIoUWzZZCV2AocI6ZfRHaegH3A+ebWQZwXug5wExg\nBbAcGA/8Jop1C96QIfDzzzBrVqxrIiJSaOWiVbBzbgFgefz43Fxe74CR0apP1F1wgU978cor0Lt3\nrGsjIlIouqM5KJFpL7ZujXVtREQKRUEhSEOHKu2FiJRoCgpB6twZWrXSLCQRKbEUFIIUTnvx0Uew\ncmWsayMiUmAKCkEbPNjvlfZCREogBYWgnXACnHWW0l6ISImkoBANQ4fC999DenqsayIiUiAKCtHQ\nr5/SXohIiaSgEA01asCll8LkyXDgQKxrIyKSb2UyKHz+uR8P3rs3iicZMgQ2b4Z//SuKJxERCVaZ\nDApbt8Krr8LEiVE8SY8eULMmTJoUxZOIiASrTAaFs8+GtDR4+GE4eDBKJylfHq64At56C3btitJJ\nRESCVSaDghmMGQMZGfD221E80aBBPiC8804UTyIiEpwyGRQALr8cmjWDBx6I4u0E3bpBw4a+r0pE\npAQos0GhXDkYPdqvoLlgQZROkpDgM6fOmgVbtkTpJCIiwSmzQQHg6quhTh146KEonmTwYMjMhKlT\no3gSEZFglOmgULky3HST7/L/5psonaRjR2jdWrOQRKREKNNBAWDkSKhUyc9EigozP+A8fz6sWhWl\nk4iIBKPMB4U6deDaa31S07Vro3SSgQP9fsqUKJ1ARCQYZT4oANx+u79f4bHHonSC5s3h1FM1C0lE\n4p6CAn5qar9+8MwzsH17lE4yaBB88UUUBy9ERIpOQSFkzBgfEJ57Lkon6N/fT1FVa0FE4piCQkha\nGpxzDjzyCOzfH4UT1KsH553ng4IW3xGROKWgEGHMGD/YHLUv84MGwQ8/+DvmRETikIJChB49oH17\nPz01KysKJ+jTBypUUBeSiMStYwYFM2tuZhVCj882s1vMLDn6VSt+ZnDnnbB0qc9MEbjq1eGSS+C1\n1/xdziIicSY/LYWpwEEzawE8BzQGSu1X3f79oXFjePDBKJ1g0CDYuBHmzo3SCURECi8/QSHLOZcJ\n9AGecM6NAY6LbrViJykJbrvN34Acla7/nj39cp2TJ0ehcBGRoslPUDhgZgOBq4B/ho4lRa9KsTd8\nOCQnRylRXsWK0LcvTJsGe/ZE4QQiIoWXn6BwDXA6cJ9z7gczawq8fKw3mdkEM9toZl9HHLvbzNaY\n2RehrVfEz35nZsvN7Dsz61GYiwlKtWrwm9/4z+2MjCicYNAg2LED3n03CoWLiBTeMYOCc+4b59wt\nzrnJZlYTqOaceyAfZf8DuDCX44845zqFtpkAZtYGGAC0Db3n72aWmO+riIKbb/Yrao4bF4XCzz4b\njjtOs5BEJO7kZ/bRv82supnVAhYD483smB+Vzrn5wM/5rEdvYIpzbp9z7gdgOdAln++Nivr1Ydgw\nePFF2LAh4MITE2HAAN9S2Lo14MJFRAovP91HNZxz24HLgYnOuVOB84pwzpvMbEmoe6lm6FhDIDKv\n9OrQsZgaPdrf3fzkk1EofNAgX/i0aVEoXESkcPITFMqZ2XHAleQMNBfW00BzoBOwDvhbQQswsxFm\nlm5m6Zs2bSpidY7upJPgssvgqadg586AC+/cGVq2VBeSiMSV/ASFe4D3gP855z4zs2ZAoYZfnXMb\nnHMHnXNZwHhyuojW4O9/CGsUOpZbGc8559Kcc2kpKSmFqUaB3Hkn/PILTJgQcMHhxXc++ADWrQu4\ncBGRwsnPQPMbzrkOzrkbQ89XOOf6FuZkoRZHWB8gPDPpbWCAmVUIzW5qCSwszDmCdtppcMYZ8Le/\nwYEDARc+cKBPjvfaawEXLCJSOPkZaG5kZtND00s3mtlUM2uUj/dNBj4BTjKz1WZ2HfCgmX1lZkuA\n7sBtAM65pcDrwDfAbGCkc+5gEa4rUHfeCT/9BG+8EXDBJ53ku5HUhSQiccLcMdI4m9kcfFqL8L0J\nQ4DBzrnzo1y3Y0pLS3Pp6elRP09WFrRtC5UrQ3q67/kJzLhxfkT7++/9GIOISJSZ2SLnXFpuP8vP\nmEKKc+5F51xmaPsHEP3O/DiSkACjRsHixbBgQcCF9+/vo4zSXohIHMhPUNhiZkPMLDG0DQG2RLti\n8WboUKhVyy/CE6iGDf3NbFp8R0TiQH6CwrX46ajr8dNI+wFXR7FOcalyZbjhBnjrLVixIuDCBw2C\n776Dzz8PuGARkYLJz+yjH51zlzrnUpxzdZ1zlwGFmn1U0o0c6W9GfuKJgAvu29enZ9WAs4jEWGFX\nXrs90FqUEA0b+iGAF16A7dsDLLhmTejVy48rHIybSVciUgYVNigEOf+mRBk1yic4DfxmtkGD/ALR\n8+cHXLCISP4VNiiU2RHRtDR/M9vjjwf8pf7ii6FqVc1CEpGYyjMomNkOM9uey7YDaFCMdYw7t90G\nP/wAM2YEWGjlytC7N7z5pk+UJyISA3kGBedcNedc9Vy2as65csVZyXjTuzc0aRKF6akDB/pES++/\nH3DBIiL5U9juozItMRFuucXfyBboDdXnn+9vhlAXkojEiIJCIV13nV+289FHAyy0fHno18/3S+3e\nHWDBIiL5o6BQSNWrw7XX+gSna3JN8l1IAwfCrl3wzjsBFioikj8KCkVwyy1+BtJTTwVYaLdu0KCB\nupBEJCaONvuosZlNMbOPzOwuM0uK+NlbxVO9+NasmV+Z7dln/Zf7QCQm+jvkZs3S+s0iUuyO1lKY\nAPwbuBk4DvjQzGqHfnZClOtVYtx+O/z8M7z0UoCFDhyo9ZtFJCaOFhRSnHPPOOe+cM7dDPwdmG9m\nzSnDN68drmtX6NLFT08N7Ga2tDRo3lxdSCJS7I4WFJLMrGL4iXPuFeBW/HrNx+X5rjLGzK+Rs3w5\n/POfARY6cKBfv3n9+oAKFRE5tqMFheeBUyMPOOf+BVxBztrKAlx+OZxwgl/HOTADB/ol3wJfA1RE\nJG9Hu6P5Eefch7kc/xx4N6q1KmHKlYNbb4WPPoLPPguo0DZtoEMHdSGJSLFS6uyAXHedv3dh3LgA\nCx0wAD75BFauDLBQEZG8KXV2QKpXh+uv9709P/0UUKEDBvj9lCkBFSgicnRKnR2gW27x+8ceC6jA\npk3htNPUhSQixUapswN0/PFw5ZUwfjxs2xZQoQMHwpIl8M03ARUoIpI3pc4O2O23+5XZXnghoAKv\nvBISErR+s4gUC+U+ClhaGpx5pu9CyswMoMD69X1K7X/8I6ACRUTypqAQBaNH+8HmN98MqMBf/9qn\nYn1XM4FFJLoUFKLg4ouhZUt/M5sLYkj+4ot95tRnnw2gMBGRvCkoREFCgh9bSE+H+fMDKLBcORg+\nHGbP9otDi4hEiYJClFx1FaSkwEMPBVTg8OE+J9L48QEVKCJyJAWFKKlUCW66yQ8DLF0aQIGNG/tu\npBde8Gm1RUSiIGpBwcwmmNlGM/s64lgtM5tjZhmhfc3QcTOzx81suZktMbPUaNWrOI0cCZUrw8MP\nB1Tgr38NGzfCW1rjSESiI5othX8AFx52bCww1znXEpgbeg7QE2gZ2kYAT0exXsWmdm2/jvOkSQGt\n43zBBdCkCTzzTACFiYgcKWpBwTk3H/j5sMO9gfAaZS8Bl0Ucn+i8/wLJZlYq1my4/Xa/+E4gqS8S\nE2HECJg3D777LoACRUQOVdxjCvWcc+tCj9cD9UKPGwKrIl63OnTsCGY2wszSzSx906ZN0atpQJo2\nhSuu8LNJt28PoMBrrvGzkZ57LoDCREQOFbOBZuecoxCJ9Zxzzznn0pxzaSkpKVGoWfDGjPEBIZDP\n8fr1oU8ff4fznj0BFCgikqO4g8KGcLdQaL8xdHwN0DjidY1Cx0qFzp3hnHPg0UcDmjj061/Dzz8H\neMu0iIhX3EHhbeCq0OOrgBkRx4eFZiGdBmyL6GYqFcaM8YPNgWTB7t4dTjxRA84iErhoTkmdDHwC\nnGRmq83sOuB+4HwzywDOCz0HmAmsAJYD44HfRKtesdKjB7Rv76enFjn1hRnccAN8/LFPqy0iEhBz\ngSTniY20tDSXnp4e62rk28svw7BhMHMm9OxZxMK2bIGGDf06oE89FUj9RKRsMLNFzrm03H6mO5qL\n0YAB0KgRPPhgAIXVru3XWpg4McAVfUSkrFNQKEZJSXDbbfDvf8PChQEUeOutsHMnPP98AIWJiCgo\nFLvrr4dateC++wIorHNnOOssePxxLcAjIoFQUChm1arBqFHw9tvw5ZcBFHjbbX5Fn2nTAihMRMo6\nBYUYuPlmqF4d/vrXAAq7+GJo0QLGjQugMBEp6xQUYiA52afVfuMN+PbbIhaWmOibHp9+Cp98Ekj9\nRKTsUlCIkVGj/JoL/+//BVDY1VdDzZpqLYhIkSkoxEhKis9WMWkSrFhRxMKqVPHZU6dN03KdIlIk\nCgoxNHq0T3j6wAMBFHbTTX5x6CeeCKAwESmrFBRiqEEDf0Pyiy/C6tVFLKxRI+jf39+zoJvZRKSQ\nFBRi7M47fS6khx4KoLDbboMdO/w6ziIihaCgEGMnnODzIT33HGzYUMTCOneGM8/UzWwiUmgKCnFg\n7Fi/zkIgk4duvx1+/BGmTw+gMBEpaxQU4kDLlj5Z3t//7pOfFoluZhORIlBQiBN33eVz2xX5szwx\n0Y8t/Pe/8K9/BVI3ESk7FBTiRNu2vrXw2GMBjC1cdx0cf7yPNCV4vQwRKX4KCnHkz3+GvXsDuMu5\nQgVf2GefaWxBRApEQSGOnHiiz1jx9NM+8WmRDB0KrVvD//2fZiKJSL4pKMSZP/7R7//ylyIWlJgI\n997rM+69/HKR6yUiZYOCQpw5/nifE+nFFyEjo4iF9ekDp5wCd9/t+6VERI5BQSEO3XWXHxb405+K\nWJCZH6D46Sd45plA6iYipZuCQhyqV88vvzx5MixZUsTCzj3Xb/fd51NgiIgchYJCnBozBmrUgD/8\nIYDC/vpX2LwZHnkkgMJEpDRTUIhTNWv6ZHlvv+3vQyuSLl38+MLDD/vgICKSBwWFOHbLLVC3rp9V\nWmT33gu7dgW01JuIlFYKCnGsalU/6PzBBzB3bhELa9PGp2N96imfME9EJBcKCnHuhhugcWM/8Fzk\nWaV//rNf6u2GG5T+QkRypaAQ5ypW9GstLF0Kv/99EQs7/ni/9ud772khHhHJlYJCCXDhhXDjjT6D\n6ocfFrGwG2+Es8/26y4UOZeGiJQ2CgolxEMPQfPmcNVVsH17EQpKSIAJEyArC4YPVzeSiBwiJkHB\nzFaa2Vdm9oWZpYeO1TKzOWaWEdrXjEXd4lWVKjBxIqxa5ZdLKJKmTX2UmTMHxo8PpH4iUjrEsqXQ\n3TnXyTmXFno+FpjrnGsJzA09lwinn+6X7pwwAWbMKGJhN9wA55wDo0drNpKIZIun7qPewEuhxy8B\nl8WwLnHrT3+CTp3g+uth48YiFJSQkDPYfN116kYSESB2QcEB75vZIjMbETpWzzm3LvR4PVAvtzea\n2QgzSzez9E2bNhVHXeNK+fI+E/a2bQHMLG3SxN/lPHcuPPtsUFUUkRIsVkHhDOdcKtATGGlmZ0b+\n0Dnn8IHjCM6555xzac65tJSUlGKoavxp186nM3rrLXjppWO//qhGjIDzzoM77oAffgikfiJScsUk\nKDjn1oT2G4HpQBdgg5kdB+ToRC0AABFNSURBVBDaF6VzpNQbNQrOPBN+8xv49NMiFGTmu5ESE+Gi\ni6AMtr5EJEexBwUzq2Jm1cKPgQuAr4G3gatCL7sKKOpQaqmWmAivvw4NGvjP8u++K0Jhxx/vM+/9\n8AP06OH7pkSkTIpFS6EesMDMvgQWAu8652YD9wPnm1kGcF7ouRxFvXr+5uTERP9ZvnZtEQo76yyY\nNg2+/tpHmV27AquniJQc5krwrJO0tDSXnp4e62rE3KJF/iblZs1g/ny/DkOhvfkm9O/vF+Z55x2/\nBJyIlCpmtijidoBDxNOUVCmkzp39l/xly6B37yImzuvXz48xzJkDAwZAZmZg9RSR+KegUEqcfz78\n4x8+N9LQoXDwYBEKu/pqePxxP73pmmt8SgwRKRPKxboCEpxBg2D9en+T8s03w5NP+nvUCuXmm/2a\nzv/3f76QZ5/1KVtFpFRTUChlbr/dB4aHHvIzkiZOhIYNC1nY737nmxx//KPP3T11KpxwQqD1FZH4\nou6jUuiBB+D55/3azh06wPTphSzIDP7wB59oKSPDD17MmRNoXUUkvigolEJmPp3R55/7hKiXX+5v\nXC70LNNLL4X0dDjuOD/39a9/1TiDSCmloFCKnXgifPwx/Pa3vuXQuTMsXlzIwlq29E2PgQP9OEOf\nPrB1a6D1FZHYU1Ao5cqXh/vv9znvdu6EU0/196Y9/3whsqxWqQKvvOJnJs2c6dO1TpmiDKsipYiC\nQhnRvTt8+aXPmbRsmU+9Xb8+dOvml/nMdy48Mz8z6cMPITnZtxxOPdXfNSciJZ7uaC6DnIMlS/wA\n9PTp/jFA5cpQp47fUlJyHleq5IcQDh48dF+tShaddszn5Kl/oPmG/5Bw6SV+lLtVq9heoIgc1dHu\naFZQEFasgH/+E376CTZv9olSN2/Oebxvn79VISHB51kK73fsgAMHfBnVKuyjY+YiTs5aRPszkqnf\n/0xSOp+QHWCqV/eNjG3b/OzWr7/O2S9b5nPy9ezpt7S03O+v2LvXj4n85z+werVfDqJFC792dbNm\nuo1CJL8UFCQq9u/3H+yffx7aFu7ni8WOXZlH5ktKSvKBYcuWnGNVqkDbtr5h8d13sHChb8XUqQMX\nXOADRNWqPgh8/LGfALV/v39v5cqwe/eh52jUyA+un3ce9Orlp+OaRfEXIFJCKShIsTl4EFZ9sYVN\nL89m07SP2LxqN5vKN2JTq25sbZZKky51adfeaNfOtw4iWwSbN8P778OsWT77a3hph/Llfeuha1f4\n1a/8lpLiX/+//x26LVkCX3zh39ewoQ8sF13k8/tVq1b8vw+ReKSgILHhHHzyiZ/q9Npr/qt9/fq+\nGdCjh/9KX7durm/NyvJdRQcOQGpqwZK1rlsHs2f7CVLvvw/bt/uWSufOPrCEt8hTO+dbK//5T07L\npHx5f3/HsGG+lSNSWigoSOxt3+5Tuc6e7e+K/vlnfzw11QeIbt38p3YeQaKwDhzwH/CzZsGCBfDZ\nZzldUC1a+FbH1q3+NZs3++O1a/vj69f711et6gPDyJHQpk2g1cs35/zYz5o1cMYZRchpJYKCgsSb\ngwd9M+C99/xX+U8+yUnR3bix7ysKbyef7PuKArJ3r19/Itwi+O9//foTXbv6D9uuXeGkk3LGIhYu\nhKee8g2dffv81N5rr4X27f0Ad9WqgVXtCD/+CPPm5WyrVvnjHTrAPff4G801ZiKFoaAg8W3HDh8k\nFi3yo8np6T7XUljdutCu3aFbq1b+Poli+lTctMkvM/H0036WVli9ej44tGjh49nOnX4wfcsW3/LY\nssXPuDr9dH9vSK9eUC6PNJTO+Ut/5RW/vlH43pE6dfwiSt27+wH2v/7V/3rS0uDee31vnIKDFISC\ngpQ8W7f6QLFkiZ+3Gt4iEzhVreo/iSO3448/dF+pUqDVysyEr76C5cv9wHbkfs0aX6Xatf1Wp47f\nV6rkp/xu2ODX1L7mGp+bqmlTX+aKFTBpkg8G33/vx0969PCD4927+xlakd1FmZnw8svw5z/71sQZ\nZ/hEtg0b+mGbw7ft2/2vc9s2vw9vKSk+2Jx9tg9shQkszvkxnFWr/H79er9ft85fb8OGvn5nnOH/\nSYrD/v2+qzA8G02OpKAgpUNWlv+a/vXXflR41apDt/Xrj3xPnTo5AaJBA5/UL3KrX9+3RPL6+l7A\n6uXV13/gALz7Lowf74dVsrL8OPuePb4bC/yH85Ah0LevbwQdy/79vvVy7735W5+7QgVfbnKy7zL7\n8Uf/wQ3+AzQcINLSfDBLTvbThiODxdatvjXz6ae+a+3TT3PKCDPzv9K6dWHlSt8QBP9PEO6m69bN\nN/iONTYSngCwZIkPom3a+Drl9ruYMwfeeMMn9d261dejf3+f6DfosaCsLP/nVqeOn5BQ0igoSNmw\nf7+/qy0cJH76KWf/00/+62vkjRJhCQm+H+i443ICR4MG/litWlCzpt+Ht+rVizTSu2oVvPgivPSS\nb0UMGeIXSCrsN+k9e3zAOXjQdy9FbpUq+QBQo8aRN/eFP3DnzYN//9tvh+fDSkzMCSRmvkUUdtJJ\nPsNJWpr/wA7H2cgYe/Cgb1ktWOC3jz7KCWDJyT44nHmm31JT/ft++gk++MDn6/rggyMDXtOmOb2I\nzZr5MmfM8C2hGjX8krR9+viA9eSTvnHZr58PDu3bH/v3mZnpywpvW7bktAYzMnJah3v3+oDQsSN0\n6QKnnOK3k07yv7dffvGtwBUr/OtXrPDHKlf2gS28r1LFtzDDgTS8JSdHb0KBgoJI2L59/qttuI8j\nvK1de+h+48a8E/0lJOQEitq1D92npORsdevmPK5Uyc+LTUws3ustAOfg22/9h/jhXU1bt/qY26lT\nzgdgflozuZ1j5UofID780KfMCg8fVanif1UrV/rnKSlwzjm+G61z55xGYnj77jv/AZ6cDJddBldc\n4V8bOX15yxaf2+uJJ3yLpW9fn0p+82b/TT/c3bV+vR832ro17xTzFSr4saMWLXzS4BNO8K2tzz7z\nw2E7d/rXVavmg9svvxz6/rp1/Z/J7t3+HLt2+YCel3Ll/HuaNfPnC28nnujrULlywX//YQoKIgV1\n4ID/RPnlFz99Nrdty5ZD95s353wy5CUhwQeHpCT/NbNGjZxurMgtPBhRsaLfR27Vq/uvlnEcYApi\n3Tr/bX/+fP+4Wzf/4d6u3dHHOfbv9wGkSZNjd+H8/DM8+ig89pgfYwH/oRv5K09J8bG+Ro2cLrYa\nNfyxpk19F1te39wPHvQB9bPPfPdaVlZO+pXmzf37c7t5MivLB4bt231Q2rjx0G3dOt/KyMg4snf0\nd7/zkw4KQ0FBpLjs3ZuTNCpy27vXf4odOOC38ONffsn5yrp+fe7dW3mpUsV/0lSv7vdVq+a+hYNJ\nOMBE7itUyH0f3ipUCGS8JV5s3+677+rX9x/2Jel+jx07fNfV99/7INGli595VhgKCiIlxf79vnvr\n5599INmz59Bt927/6bBjh/+EC++3b/f9ETt3HrkVdZW8xMRDg8ThjytU8C2fcuX8lpiY87h8+dy3\nvFo5ZjmtqPLlcx6Hy09MzCk//Div5kRCQu51Cm+RdQ4/PjzrY/ixWc4W+byEOlpQKD1fAURKg/Ll\nc6bXBsE53yLZsycnyETu9+3L2YcfRx4/fIt8/eH7zMwjt3CrKHLbt6/0LMxkdmgQidwf/rrwPvI1\nkemHIwNO+Hj4fZG/r/Dj66+H0aMDvyQFBZHSzCznW3eNGrGuTQ7ncv+mnZV1aBdbuJtt/37fcZ+Z\neej+4MG8z5GVdWiACr/vwIFDg1bk46ysnEVDIhcQce7QLSsrZ5/X6yOvNbJOh2/h9+ZVdmRACTPz\nY1FRoKAgIsXvaF0+FSoULAOiBKoEDbOIiEi0KSiIiEi2uAsKZnahmX1nZsvNbGys6yMiUpbEVVAw\ns0TgKaAn0AYYaGYxymAvIlL2xFVQALoAy51zK5xz+4EpQO8Y10lEpMyIt6DQEFgV8Xx16Fg2Mxth\nZulmlr4pvIiviIgEIt6CwjE5555zzqU559JSAlyRS0RE4i8orAEib+VsFDomIiLFIK5yH5lZOeB7\n4Fx8MPgMGOScW5rH6zcBPx6j2DrA5iDrWULousuesnrtuu6CO8E5l2tXS1zd0eycyzSzm4D3gERg\nQl4BIfT6Y/YfmVl6XomfSjNdd9lTVq9d1x2suAoKAM65mcDMWNdDRKQsircxBRERiaGyEBSei3UF\nYkTXXfaU1WvXdQcorgaaRUQktspCS0FERPJJQUFERLKV6qBQVjKumtkEM9toZl9HHKtlZnPMLCO0\nrxnLOkaDmTU2s3lm9o2ZLTWzW0PHS/W1m1lFM1toZl+GrvvPoeNNzezT0N/7a2ZWPtZ1jQYzSzSz\nz83sn6Hnpf66zWylmX1lZl+YWXroWFT+zkttUChjGVf/AVx42LGxwFznXEtgbuh5aZMJjHbOtQFO\nA0aG/o1L+7XvA85xznUEOgEXmtlpwAPAI865FsAvwHUxrGM03Qosi3heVq67u3OuU8S9CVH5Oy+1\nQYEylHHVOTcf+Pmww72Bl0KPXwIuK9ZKFQPn3Drn3OLQ4x34D4qGlPJrd97O0NOk0OaAc4A3Q8dL\n3XUDmFkj4CLg+dBzowxcdx6i8ndemoPCMTOulnL1nHPrQo/XA/ViWZloM7MmwMnAp5SBaw91oXwB\nbATmAP8DtjrnMkMvKa1/748CdwJZoee1KRvX7YD3zWyRmY0IHYvK33nc3dEswXPOOTMrtXOPzawq\nMBUY5ZzbbhGLwpfWa3fOHQQ6mVkyMB1oFeMqRZ2ZXQxsdM4tMrOzY12fYnaGc26NmdUF5pjZt5E/\nDPLvvDS3FMp6xtUNZnYcQGi/Mcb1iQozS8IHhEnOuWmhw2Xi2gGcc1uBecDpQHIoqSSUzr/3rsCl\nZrYS3x18DvAYpf+6cc6tCe034r8EdCFKf+elOSh8BrQMzUwoDwwA3o5xnYrT28BVocdXATNiWJeo\nCPUnvwAsc86Ni/hRqb52M0sJtRAws0rA+fjxlHlAv9DLSt11O+d+55xr5Jxrgv///IFzbjCl/LrN\nrIqZVQs/Bi4AviZKf+el+o5mM+uF74MMZ1y9L8ZVigozmwycjU+luwH4E/AW8DpwPD69+JXOucMH\no0s0MzsD+Aj4ipw+5rvw4wql9trNrAN+YDER/8XudefcPWbWDP8NuhbwOTDEObcvdjWNnlD30R3O\nuYtL+3WHrm966Gk54FXn3H1mVpso/J2X6qAgIiIFU5q7j0REpIAUFEREJJuCgoiIZFNQEBGRbAoK\nIiKSTUFBJBdmdjCUkTK8BZZUz8yaRGa0FYknSnMhkrs9zrlOsa6ESHFTS0GkAEJ57R8M5bZfaGYt\nQsebmNkHZrbEzOaa2fGh4/XMbHpo7YMvzexXoaISzWx8aD2E90N3JmNmt4TWh1hiZlNidJlShiko\niOSu0mHdR/0jfrbNOdceeBJ/xzzAE8BLzrkOwCTg8dDxx4EPQ2sfpAJLQ8dbAk8559oCW4G+oeNj\ngZND5fw6Whcnkhfd0SySCzPb6ZyrmsvxlfgFblaEkvGtd87VNrPNwHHOuQOh4+ucc3XMbBPQKDLt\nQijN95zQ4iiY2W+BJOfcvWY2G9iJT1PyVsS6CSLFQi0FkYJzeTwuiMjcPAfJGd+7CL9iYCrwWUT2\nT5FioaAgUnD9I/afhB5/jM/cCTAYn6gP/DKJN0L2wjg18irUzBKAxs65ecBvgRrAEa0VkWjStxCR\n3FUKrWwWNts5F56WWtPMluC/7Q8MHbsZeNHMxgCbgGtCx28FnjOz6/AtghuBdeQuEXglFDgMeDy0\nXoJIsdGYgkgBhMYU0pxzm2NdF5FoUPeRiIhkU0tBRESyqaUgIiLZFBRERCSbgoKIiGRTUBARkWwK\nCiIiku3/A7J86SpLgJeaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Loss :  9.79475027726448\n",
            "\n",
            "Validation Loss :  13.860932504828817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6yomL6R3J6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict output for test dataset.\n",
        "\n",
        "Y_p = model.predict(X_test.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWWtN0nL3Sfn",
        "colab_type": "code",
        "outputId": "e5d8aeb2-83de-4a1f-c876-bf6b0c92bd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate Loss for test dataset\n",
        "\n",
        "print(model.compute_cost(Y_p,Y_test.reshape(1,102)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.63764014065143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}